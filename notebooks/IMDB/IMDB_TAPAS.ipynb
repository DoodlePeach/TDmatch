{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265173,
     "status": "ok",
     "timestamp": 1634806360258,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "NvDNRuvAKPuQ",
    "outputId": "c192846e-e1c9-4e83-fc18-6df685a4b0a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n",
      "Collecting torch-scatter==2.0.2\n",
      "  Downloading torch_scatter-2.0.2.tar.gz (22 kB)\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-scatter: filename=torch_scatter-2.0.2-cp37-cp37m-linux_x86_64.whl size=7755966 sha256=05a577c68bd9a94c40dbc165e3da3dbec8d0a4f1e19137d02f4d7aad0ae8f116\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/94/3a/203b4e701bab9db29f54b9462e62de45e54b0bf63b4ea5ced6\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter==2.0.2 -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19916,
     "status": "ok",
     "timestamp": 1634806380169,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "WLyIY_XRRwpN",
    "outputId": "2711b9c6-d9a4-42cd-d8e2-25e1f86a725f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'transformers': No such file or directory\n",
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 86970, done.\u001b[K\n",
      "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
      "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
      "remote: Total 86970 (delta 36), reused 22 (delta 6), pack-reused 86884\u001b[K\n",
      "Receiving objects: 100% (86970/86970), 70.10 MiB | 32.79 MiB/s, done.\n",
      "Resolving deltas: 100% (62566/62566), done.\n",
      "Processing ./transformers\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 7.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 66.7 MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 61.2 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 5.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.12.0.dev0-py3-none-any.whl size=2993841 sha256=20ae1fb7b2e8b30239b6d95b9f410af9d43085c65eb047917005525dd9c3dc13\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k1i1bat2/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
      "Successfully built transformers\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0.dev0\n"
     ]
    }
   ],
   "source": [
    "! rm -r transformers\n",
    "! git clone https://github.com/huggingface/transformers.git\n",
    "! cd transformers\n",
    "! pip install ./transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF-OReITX2h4"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_77v3P7X5Nk"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load(open('../../data/imdb/imdb_reviews_1000film.df','rb'))\n",
    "ground_truth = pickle.load(open('../../data/imdb/imdb_GT.pkl','rb'))\n",
    "review_ids = pickle.load(open('../../data/imdb/imdb_reviewIDs.pkl','rb'))\n",
    "row_ids = pickle.load(open('../../data/imdb/imdb_movieIDs.pkl','rb'))\n",
    "id_revs = inv_map = {v: k for k, v in review_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKwLa5IVX8Bf"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "\n",
    "movies_dic = {}\n",
    "with open('../../data/imdb/imdb_movielens.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        if row[12].replace('_',' ') not in movies_dic: \n",
    "            movies_dic[row[12].replace('_',' ')] = []\n",
    "            \n",
    "        temp = [r.replace('_',' ') for r in row[0:10]]\n",
    "        \n",
    "        month,year = '',''\n",
    "        if len(row[10]) > 0:        \n",
    "            month = datetime.date(1900, int(row[10][4::]), 1).strftime('%B')\n",
    "            year = row[10][0:4]\n",
    "        \n",
    "        temp.append(month.lower() + ' ' + year)\n",
    "        temp.append(int(float(row[14])))\n",
    "        \n",
    "        movies_dic[row[12].replace('_',' ')].append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8mSr7mqZYyy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imdb_tables = []\n",
    "header = ['id','info']\n",
    "\n",
    "i = 0\n",
    "for m in movies_dic:\n",
    "  #for n in movies_dic[m]: \n",
    "    r = ' '.join([str(mm) for mm in movies_dic[m][0] if mm not in ['',' ']])\n",
    "    #r = m + ' ' + r\n",
    "\n",
    "    imdb_tables.append([i, r])\n",
    "    i += 1\n",
    "\n",
    "df = pd.DataFrame(imdb_tables,columns=header)\n",
    "df.to_csv('imdb_all.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtyoYO8fUjlw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.mkdir('tables')\n",
    "chunks = [imdb_tables[x:x+500] for x in range(0, len(imdb_tables), 500)]\n",
    "row_address = {}\n",
    "for i in range(0,len(chunks)):\n",
    "  df = pd.DataFrame(chunks[i],columns=header)\n",
    "  df.to_csv(f'table_{i}.csv',index=False)\n",
    "\n",
    "  for r in chunks[i]:\n",
    "    row_address[r[1]] = (f'table_{i}.csv', chunks[i].index(r)   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zt4LKFskX5At"
   },
   "outputs": [],
   "source": [
    "header = ['question','table_file','answer_coordinates','answer_text']\n",
    "table = []\n",
    "\n",
    "\n",
    "for m in ground_truth:\n",
    "    if len(ground_truth[m]) == 0 : continue\n",
    "\n",
    "    for q in ground_truth[m]:\n",
    "      if len(review_ids[q]) > 1500: continue\n",
    "\n",
    "\n",
    "      #for t in movies_dic[m]:\n",
    "      tt = ' '.join([str(mm) for mm in movies_dic[m][0] if mm not in ['',' ']])\n",
    "      #tt = m + ' ' + tt\n",
    "\n",
    "      ind = row_address[tt][1]\n",
    "      row = [review_ids[q]]\n",
    "      cor,ans = [f'({ind},{1})'],[tt]\n",
    "\n",
    "      tfile = '' + row_address[tt][0]\n",
    "\n",
    "      row.append(tfile)\n",
    "    \n",
    "      row.append(cor)\n",
    "      row.append(ans)\n",
    "\n",
    "      table.append(row)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(table,columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1634826874542,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "CpZfqvopX40M",
    "outputId": "0e34927f-0784-49d4-ab79-8e7efd32275f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 219)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "\n",
    "train.to_csv('queries_train.csv',index=False)\n",
    "test.to_csv('queries_test.csv',index=False)\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634826874780,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "CArZG1hMaitR",
    "outputId": "17493296-89eb-470b-e41d-16b4edf31d91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i watch this movie for minimum 5-6 times first...</td>\n",
       "      <td>table_46.csv</td>\n",
       "      <td>[(15,1)]</td>\n",
       "      <td>[the shawshank redemption morgan freeman jeffr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather is one of the most iconic films ...</td>\n",
       "      <td>table_81.csv</td>\n",
       "      <td>[(22,1)]</td>\n",
       "      <td>[the godfather al pacino marlon brando robert ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dark, yes, complex, ambitious. Christopher Nol...</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>[(25,1)]</td>\n",
       "      <td>[the dark knight christian bale heath ledger m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Confidently directed, dark, brooding, and pack...</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>[(25,1)]</td>\n",
       "      <td>[the dark knight christian bale heath ledger m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This if you have not already.It is violent, ac...</td>\n",
       "      <td>table_79.csv</td>\n",
       "      <td>[(6,1)]</td>\n",
       "      <td>[pulp fiction bruce willis eric stoltz phil la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>Here is a tragedy -- a great film doomed by a ...</td>\n",
       "      <td>table_246.csv</td>\n",
       "      <td>[(20,1)]</td>\n",
       "      <td>[duck you sucker james coburn rod steiger romo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>What can I say about The Jungle Book? It proba...</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>[(36,1)]</td>\n",
       "      <td>[the jungle book scarlett johansson bill murra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>There aren't many animated Disney films I don'...</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>[(36,1)]</td>\n",
       "      <td>[the jungle book scarlett johansson bill murra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>There has always been a debate regarding what ...</td>\n",
       "      <td>table_101.csv</td>\n",
       "      <td>[(6,1)]</td>\n",
       "      <td>[from here to eternity montgomery clift donna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>One of the things that made Hitchcock great wa...</td>\n",
       "      <td>table_148.csv</td>\n",
       "      <td>[(25,1)]</td>\n",
       "      <td>[lifeboat tallulah bankhead william bendix wal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>729 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  ...                                        answer_text\n",
       "0    i watch this movie for minimum 5-6 times first...  ...  [the shawshank redemption morgan freeman jeffr...\n",
       "1    The Godfather is one of the most iconic films ...  ...  [the godfather al pacino marlon brando robert ...\n",
       "2    Dark, yes, complex, ambitious. Christopher Nol...  ...  [the dark knight christian bale heath ledger m...\n",
       "3    Confidently directed, dark, brooding, and pack...  ...  [the dark knight christian bale heath ledger m...\n",
       "4    This if you have not already.It is violent, ac...  ...  [pulp fiction bruce willis eric stoltz phil la...\n",
       "..                                                 ...  ...                                                ...\n",
       "724  Here is a tragedy -- a great film doomed by a ...  ...  [duck you sucker james coburn rod steiger romo...\n",
       "725  What can I say about The Jungle Book? It proba...  ...  [the jungle book scarlett johansson bill murra...\n",
       "726  There aren't many animated Disney films I don'...  ...  [the jungle book scarlett johansson bill murra...\n",
       "727  There has always been a debate regarding what ...  ...  [from here to eternity montgomery clift donna ...\n",
       "728  One of the things that made Hitchcock great wa...  ...  [lifeboat tallulah bankhead william bendix wal...\n",
       "\n",
       "[729 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPrYJOn81f0D"
   },
   "source": [
    "## Prepare the data \n",
    "\n",
    "Let's look at the first few rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634826875199,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "2X27wyd805D8",
    "outputId": "e1f2e453-8d06-4a99-ec58-a4552e71f40f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mean, i WAS pretty high, but still.It defini...</td>\n",
       "      <td>table_280.csv</td>\n",
       "      <td>['(17,1)']</td>\n",
       "      <td>['hamilton peter stormare mats lngbacka lena o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is just a masterpiece. It is probably the...</td>\n",
       "      <td>table_250.csv</td>\n",
       "      <td>['(5,1)']</td>\n",
       "      <td>['the day of the jackal edward fox michael lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Key Largo is an absolutely brilliant film. Cas...</td>\n",
       "      <td>table_167.csv</td>\n",
       "      <td>['(0,1)']</td>\n",
       "      <td>['key largo humphrey bogart edward g robinson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Earlier to, many movies tried to capture the i...</td>\n",
       "      <td>table_558.csv</td>\n",
       "      <td>['(30,1)']</td>\n",
       "      <td>['kai po che amrita puri amit sadh rajkummar r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kokuhaku (or Confessions) is a real winner fro...</td>\n",
       "      <td>table_430.csv</td>\n",
       "      <td>['(5,1)']</td>\n",
       "      <td>['confessions masaki okada yoshino kimura tets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  ...                                        answer_text\n",
       "0  I mean, i WAS pretty high, but still.It defini...  ...  ['hamilton peter stormare mats lngbacka lena o...\n",
       "1  This is just a masterpiece. It is probably the...  ...  ['the day of the jackal edward fox michael lon...\n",
       "2  Key Largo is an absolutely brilliant film. Cas...  ...  ['key largo humphrey bogart edward g robinson ...\n",
       "3  Earlier to, many movies tried to capture the i...  ...  ['kai po che amrita puri amit sadh rajkummar r...\n",
       "4  Kokuhaku (or Confessions) is a real winner fro...  ...  ['confessions masaki okada yoshino kimura tets...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"queries_train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1634826876150,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "BAovAs5s1k10",
    "outputId": "0d46139e-11af-48e8-b00c-2452f3025de3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mean, i WAS pretty high, but still.It defini...</td>\n",
       "      <td>table_280.csv</td>\n",
       "      <td>[(17, 1)]</td>\n",
       "      <td>[hamilton peter stormare mats lngbacka lena ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is just a masterpiece. It is probably the...</td>\n",
       "      <td>table_250.csv</td>\n",
       "      <td>[(5, 1)]</td>\n",
       "      <td>[the day of the jackal edward fox michael lons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Key Largo is an absolutely brilliant film. Cas...</td>\n",
       "      <td>table_167.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[key largo humphrey bogart edward g robinson l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Earlier to, many movies tried to capture the i...</td>\n",
       "      <td>table_558.csv</td>\n",
       "      <td>[(30, 1)]</td>\n",
       "      <td>[kai po che amrita puri amit sadh rajkummar ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kokuhaku (or Confessions) is a real winner fro...</td>\n",
       "      <td>table_430.csv</td>\n",
       "      <td>[(5, 1)]</td>\n",
       "      <td>[confessions masaki okada yoshino kimura tetsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was sceptical before watching this film but ...</td>\n",
       "      <td>table_50.csv</td>\n",
       "      <td>[(22, 1)]</td>\n",
       "      <td>[joyeux noel gary lewis dany boon ian richards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This was a surprisingly intense film that I'm ...</td>\n",
       "      <td>table_207.csv</td>\n",
       "      <td>[(15, 1)]</td>\n",
       "      <td>[the experiment moritz bleibtreu christian ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How great was this movie? I fell in love with ...</td>\n",
       "      <td>table_38.csv</td>\n",
       "      <td>[(7, 1)]</td>\n",
       "      <td>[remember the titans ryan gosling denzel washi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Top 10 comedies of all time. Enough jokes and ...</td>\n",
       "      <td>table_33.csv</td>\n",
       "      <td>[(20, 1)]</td>\n",
       "      <td>[the hangover bradley cooper rob riggle mike e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>David Lean's \"Doctor Zhivago\" is a classic fil...</td>\n",
       "      <td>table_71.csv</td>\n",
       "      <td>[(25, 1)]</td>\n",
       "      <td>[doctor zhivago julie christie klaus kinski ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  ...                                        answer_text\n",
       "0  I mean, i WAS pretty high, but still.It defini...  ...  [hamilton peter stormare mats lngbacka lena ol...\n",
       "1  This is just a masterpiece. It is probably the...  ...  [the day of the jackal edward fox michael lons...\n",
       "2  Key Largo is an absolutely brilliant film. Cas...  ...  [key largo humphrey bogart edward g robinson l...\n",
       "3  Earlier to, many movies tried to capture the i...  ...  [kai po che amrita puri amit sadh rajkummar ra...\n",
       "4  Kokuhaku (or Confessions) is a real winner fro...  ...  [confessions masaki okada yoshino kimura tetsu...\n",
       "5  I was sceptical before watching this film but ...  ...  [joyeux noel gary lewis dany boon ian richards...\n",
       "6  This was a surprisingly intense film that I'm ...  ...  [the experiment moritz bleibtreu christian ber...\n",
       "7  How great was this movie? I fell in love with ...  ...  [remember the titans ryan gosling denzel washi...\n",
       "8  Top 10 comedies of all time. Enough jokes and ...  ...  [the hangover bradley cooper rob riggle mike e...\n",
       "9  David Lean's \"Doctor Zhivago\" is a classic fil...  ...  [doctor zhivago julie christie klaus kinski ge...\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def _parse_answer_coordinates(answer_coordinate_str):\n",
    "  \"\"\"Parses the answer_coordinates of a question.\n",
    "  Args:\n",
    "    answer_coordinate_str: A string representation of a Python list of tuple\n",
    "      strings.\n",
    "      For example: \"['(1, 4)','(1, 3)', ...]\"\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    answer_coordinates = []\n",
    "    # make a list of strings\n",
    "    coords = ast.literal_eval(answer_coordinate_str)\n",
    "    # parse each string as a tuple\n",
    "    for row_index, column_index in sorted(\n",
    "        ast.literal_eval(coord) for coord in coords):\n",
    "      answer_coordinates.append((row_index, column_index))\n",
    "  except SyntaxError:\n",
    "    raise ValueError('Unable to evaluate %s' % answer_coordinate_str)\n",
    "  \n",
    "  return answer_coordinates\n",
    "\n",
    "\n",
    "def _parse_answer_text(answer_text):\n",
    "  \"\"\"Populates the answer_texts field of `answer` by parsing `answer_text`.\n",
    "  Args:\n",
    "    answer_text: A string representation of a Python list of strings.\n",
    "      For example: \"[u'test', u'hello', ...]\"\n",
    "    answer: an Answer object.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    answer = []\n",
    "    for value in ast.literal_eval(answer_text):\n",
    "      answer.append(value)\n",
    "  except SyntaxError:\n",
    "    raise ValueError('Unable to evaluate %s' % answer_text)\n",
    "\n",
    "  return answer\n",
    "\n",
    "data['answer_coordinates'] = data['answer_coordinates'].apply(lambda coords_str: _parse_answer_coordinates(coords_str))\n",
    "data['answer_text'] = data['answer_text'].apply(lambda txt: _parse_answer_text(txt))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1634826880302,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "rSuU56SmJC9W",
    "outputId": "ec86adc1-d78b-4171-92a7-a604df8f1e52"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>This movie was stunning in many aspects. Visua...</td>\n",
       "      <td>table_60.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[the illusionist rufus sewell eddie marsan jak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>''Kakushi-toride no san-akunin'', better well ...</td>\n",
       "      <td>table_247.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[the hidden fortress toshir mifune takashi shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Other reviewers, aside from seeing this as the...</td>\n",
       "      <td>table_94.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[the man who shot liberty valance lee marvin w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Modern Times owes a lot to Metropolis, despite...</td>\n",
       "      <td>table_102.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[modern times paulette goddard stanley blyston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>I saw this with my family in a theatre. Superb...</td>\n",
       "      <td>table_79.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[my name is khan shah rukh khan jimmy shergill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Well... O.K.! I'm gonna say the same things th...</td>\n",
       "      <td>table_253.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[white heat james cagney virginia mayo edmond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>After reading on the BBC that this was voted t...</td>\n",
       "      <td>table_32.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[serenity adam baldwin sean maher michael hitc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>At first glance, you would think this movie wo...</td>\n",
       "      <td>table_51.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[silver linings playbook jennifer lawrence rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>***May contain spoilers***If you watched Befor...</td>\n",
       "      <td>table_95.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[before midnight seamus daveyfitzpatrick arian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>If you saw The Prestige, prepare yourself for ...</td>\n",
       "      <td>table_60.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[the illusionist rufus sewell eddie marsan jak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  ...                                        answer_text\n",
       "52   This movie was stunning in many aspects. Visua...  ...  [the illusionist rufus sewell eddie marsan jak...\n",
       "19   ''Kakushi-toride no san-akunin'', better well ...  ...  [the hidden fortress toshir mifune takashi shi...\n",
       "75   Other reviewers, aside from seeing this as the...  ...  [the man who shot liberty valance lee marvin w...\n",
       "95   Modern Times owes a lot to Metropolis, despite...  ...  [modern times paulette goddard stanley blyston...\n",
       "354  I saw this with my family in a theatre. Superb...  ...  [my name is khan shah rukh khan jimmy shergill...\n",
       "196  Well... O.K.! I'm gonna say the same things th...  ...  [white heat james cagney virginia mayo edmond ...\n",
       "492  After reading on the BBC that this was voted t...  ...  [serenity adam baldwin sean maher michael hitc...\n",
       "388  At first glance, you would think this movie wo...  ...  [silver linings playbook jennifer lawrence rob...\n",
       "252  ***May contain spoilers***If you watched Befor...  ...  [before midnight seamus daveyfitzpatrick arian...\n",
       "494  If you saw The Prestige, prepare yourself for ...  ...  [the illusionist rufus sewell eddie marsan jak...\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sort_values(by=['answer_coordinates'], ascending=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5iU5byAICWb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TapasTokenizer\n",
    "table_csv_path = ''\n",
    "\n",
    "# initialize the tokenizer\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1634826883262,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "yyQeHSg1lU41",
    "outputId": "bc4a3170-59f3-4475-9759-f4592e855a26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.iloc[4].question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJLmPPIZ0L39"
   },
   "outputs": [],
   "source": [
    "table_csv_path = ''\n",
    "item = data.iloc[1]\n",
    "\n",
    "table = pd.read_csv(table_csv_path + item.table_file).astype(str) \n",
    "\n",
    "display(table)\n",
    "print(item.question)\n",
    "\n",
    "encoding = tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text,\n",
    "                     truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-n9vDTD1-k9"
   },
   "outputs": [],
   "source": [
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        table = pd.read_csv(table_csv_path + item.table_file).astype(str) # TapasTokenizer expects the table data to be text only\n",
    "        encoding = self.tokenizer(table=table,\n",
    "                                   queries=item.question,\n",
    "                                   answer_coordinates=item.answer_coordinates,\n",
    "                                   answer_text=item.answer_text,\n",
    "                                   truncation=True,\n",
    "                                   padding=\"max_length\",\n",
    "                                   return_tensors=\"pt\"\n",
    "         )\n",
    "\n",
    "          # use encodings of second table-question pair in the batch\n",
    "        encoding = {key: val[-1] for key, val in encoding.items()}\n",
    "\n",
    "        #encoding[\"float_answer\"] = torch.tensor(item.float_answer)\n",
    "\n",
    "\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "#data = pd.read_csv('data/queries.csv')\n",
    "train_dataset = TableDataset(df=data, tokenizer=tokenizer)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1634807347774,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "X4CHgnTzwfNp",
    "outputId": "e1673371-e0ce-4553-e313-496d8bab789e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 7])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1634807348045,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "bZN1psdBy5_s",
    "outputId": "da44ed12-f5cf-4739-c3d3-a9aa4173c21e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHAyf85k_xQt"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1634807350555,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "FoqySHh-_0JV",
    "outputId": "90237b67-25e8-4266-908c-a86a615c487a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1634807350555,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "g5pjJCCT_53N",
    "outputId": "9aa45a36-9143-423a-e1be-85c31f70ff29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVb1-H-jAEoS"
   },
   "source": [
    "Let's decode the first table-question pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634806435710,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "1vfjT1JC_7zI",
    "outputId": "84f41784-5d85-4218-b494-47ca044af23c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] hero is noteworthy on at least two counts. first, there are scenes of haunting beauty ( \" duel in the yellow forest \" and \" turquoise autumn \" to site a couple ) that, like the best of impressionist paintings, are so affecting that you will forever see the world in a slightly different way having once beheld them. secondly, the overall message of the film is a provocative one. the claim is that a degree of human casualties and suffering may be the optimal path to a better world, especially when the alternative is equally brutal chaos. this is not a popular theme. it has become much more fashionable to be anti - war in all cases. and understandably so, since variations of this logic have often been used in the past to justify atrocities. but the film provides a crisp litmus test for avoiding delusion : action must be taken with a heart void of malice and an unwavering commitment to the broadest possible ultimate outcome of good for all. can anyone live up to this standard? several characters in the movie do, each in their own way. if the standard could be met, would the world be a better place? these are questions worth reflecting on that have not been dealt with, to this depth, in any film i\\'m aware of. [SEP] id info 2500 jet 2501 dustin 2502 jet 2503 sanjeev 2504 fisher 2505 katherine 2506 james 2507 hugo 2508 james 2509 tony 2510 kate 2511 omar 2512 dany 2513 mekhi 2514 eminem 2515 heath 2516 heath 2517 julian 2518 jackie 2519 bruce 2520 bruce 2521 robert 2522 sam 2523 joel 2524 owen 2525 martin 2526 ralph 2527 jaden 2528 jennifer 2529 christian 2530 ryan 2531 sandra 2532 bruce 2533 tommy 2534 lea 2535 michael 2536 scarlett 2537 rachel 2538 scarlett 2539 jennifer 2540 dakota 2541 sylvester 2542 antonio 2543 kevin 2544 matthew 2545 tom 2546 steve 2547 clifton 2548 jerry 2549 mark 2550 tom 2551 tom 2552 javier 2553 patrick 2554 andrew 2555 jesse 2556 carmen 2557 ben 2558 beau 2559 anna 2560 kelly 2561 david 2562 lea 2563 michael 2564 will 2565 will 2566 keanu 2567 jack 2568 meryl 2569 meryl 2570 harrison 2571 chadwick 2572 philip 2573 matt 2574 bill 2575 jim 2576 paul 2577 paul 2578 robert 2579 dominique [PAD] [PAD]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sujsp8o9DtsY"
   },
   "outputs": [],
   "source": [
    "#first example should not have any prev_labels set\n",
    "assert batch[\"token_type_ids\"][0][:,3].sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIeql5vfFI6s"
   },
   "source": [
    "Let's decode the second table-question pair and verify some more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1634806435710,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "WrNo_qMqFOzi",
    "outputId": "f9f68feb-d167-45f5-a081-29a99faa854c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] a scorching performance from bogey makes this film a real classic, his dixon steele one of the great screen characters. in this more biting version of the plot of hitchcock's suspense / comedy suspicion, bogart is a kind and loving screenwriter with a violent streak of temper waiting to break out and a taste for a drink or two, wooing gloria grahame's pretty young actress next door. the death of a young girlfriend of his hangs over him throughout the movie, as graham at first believes him to be innocent, then later, having fallen for his charms, begins to suspect he may have had something to do with the girl's death after all, as his temper becomes more and more uncontrolled and frightening. the police circle around, making his nervous anger worse ; the relationship begins to crumble into a mess of fear, lies and misunderstanding. through all this dixon steele emerges as a great and brilliant creation, a highlight even in a career as illustrious as that of bogart, a charming and witty man when happy, a black and vengeful man when roused to anger, a man of contradictions that only seems the more real, heroic, and ultimately tragic. bogart's performance is brilliant, but the setting works well too, grahame is great as the sassy girl he falls for, then frightens, the story chugs along at a fair lick, but allowing plenty of time for the many fun minor characters to develop well, and the script is a corker - wonderful stuff. [SEP] id info 14700 humphrey 14701 burt 14702 winona 14703 john 14704 jeff 14705 roberto 14706 jeanne 14707 kouichi 14708 billy 14709 markku 14710 adam 14711 bill 14712 molly 14713 mark 14714 philippe 14715 sren 14716 peter 14717 edward 14718 oskar 14719 jeff 14720 lee 14721 beau 14722 billy 14723 michael 14724 kirstie 14725 kathleen 14726 goldie 14727 jeanpaul 14728 kim 14729 grard 14730 grard 14731 woody 14732 bette 14733 lora 14734 ann 14735 melina 14736 michael 14737 julie 14738 yuliya 14739 joseph 14740 patrick 14741 javier 14742 nan 14743 jean 14744 rufus 14745 paul 14746 larry 14747 peter 14748 steve 14749 toms 14750 victoria 14751 michael 14752 eddie 14753 guy 14754 lucinda 14755 lucinda 14756 keiko\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4PRdYvBE1k3"
   },
   "outputs": [],
   "source": [
    "for id, prev_label in zip(batch[\"input_ids\"][1], batch[\"token_type_ids\"][1][:,3]):\n",
    "  if id != 0:\n",
    "    print(tokenizer.decode([id]), prev_label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAem9QnIxoKb"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "Here we initialize the model with a pre-trained base and randomly initialized cell selection head, and move it to the GPU (if available).\n",
    "\n",
    "Note that the `google/tapas-base` checkpoint has (by default) an SQA configuration, so we don't need to specify any additional hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2050,
     "status": "ok",
     "timestamp": 1634826891901,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "_OsPodbiDliR",
    "outputId": "a9912b14-8308-4e0e-d9f2-9e88d3efc19c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base and are newly initialized: ['output_weights', 'column_output_bias', 'column_output_weights', 'output_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TapasForQuestionAnswering(\n",
       "  (tapas): TapasModel(\n",
       "    (embeddings): TapasEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings_0): Embedding(3, 768)\n",
       "      (token_type_embeddings_1): Embedding(256, 768)\n",
       "      (token_type_embeddings_2): Embedding(256, 768)\n",
       "      (token_type_embeddings_3): Embedding(2, 768)\n",
       "      (token_type_embeddings_4): Embedding(256, 768)\n",
       "      (token_type_embeddings_5): Embedding(256, 768)\n",
       "      (token_type_embeddings_6): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.07, inplace=False)\n",
       "    )\n",
       "    (encoder): TapasEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): TapasPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.07, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TapasForQuestionAnswering\n",
    "\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtvkIFkCzdsg"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Let's fine-tune the model in well-known PyTorch fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1008160,
     "status": "ok",
     "timestamp": 1634827901274,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "HyEZVmbdzWV9",
    "outputId": "d286daad-5fba-4734-ee1f-59bf47313f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 1.3265039920806885\n",
      "Loss: 0.5520153045654297\n",
      "Loss: 0.20379555225372314\n",
      "Loss: 0.19696374237537384\n",
      "Loss: 0.13729920983314514\n",
      "Loss: 0.1245574802160263\n",
      "Loss: 0.12099535018205643\n",
      "Loss: 0.11510320007801056\n",
      "Loss: 0.11508418619632721\n",
      "Loss: 0.11508648842573166\n",
      "Loss: 0.11552755534648895\n",
      "Loss: 0.11610338091850281\n",
      "Loss: 0.11270591616630554\n",
      "Loss: 0.11529648303985596\n",
      "Loss: 0.11626183986663818\n",
      "Loss: 0.11623670160770416\n",
      "Loss: 0.11624813079833984\n",
      "Loss: 0.1168031245470047\n",
      "Loss: 0.11669737100601196\n",
      "Loss: 0.11594832688570023\n",
      "Loss: 0.11562749743461609\n",
      "Loss: 0.11483243107795715\n",
      "Loss: 0.11452944576740265\n",
      "Loss: 0.11433851718902588\n",
      "Loss: 0.11450556665658951\n",
      "Loss: 0.11567233502864838\n",
      "Loss: 0.11503340303897858\n",
      "Loss: 0.1158408373594284\n",
      "Loss: 0.11694205552339554\n",
      "Loss: 0.11597777903079987\n",
      "Loss: 0.11490742117166519\n",
      "Loss: 0.11334699392318726\n",
      "Loss: 0.11324946582317352\n",
      "Loss: 0.11298384517431259\n",
      "Loss: 0.11273432523012161\n",
      "Loss: 0.11309194564819336\n",
      "Loss: 0.11281004548072815\n",
      "Loss: 0.11247087270021439\n",
      "Loss: 0.1102941483259201\n",
      "Loss: 0.11104277521371841\n",
      "Loss: 0.11004285514354706\n",
      "Loss: 0.12183822691440582\n",
      "Loss: 0.12173724174499512\n",
      "Loss: 0.11667925119400024\n",
      "Loss: 0.1152758002281189\n",
      "Loss: 0.1181664690375328\n",
      "Loss: 0.11710239201784134\n",
      "Loss: 0.1156090572476387\n",
      "Loss: 0.1153736412525177\n",
      "Loss: 0.1156805008649826\n",
      "Loss: 0.1162908747792244\n",
      "Loss: 0.1148509755730629\n",
      "Loss: 0.11588539183139801\n",
      "Loss: 0.11630278825759888\n",
      "Loss: 0.1147267296910286\n",
      "Loss: 0.11457688361406326\n",
      "Loss: 0.11379145085811615\n",
      "Loss: 0.1133866086602211\n",
      "Loss: 0.11346009373664856\n",
      "Loss: 0.11381229013204575\n",
      "Loss: 0.11397989094257355\n",
      "Loss: 0.11468980461359024\n",
      "Loss: 0.11446063220500946\n",
      "Loss: 0.11445246636867523\n",
      "Epoch: 1\n",
      "Loss: 0.11882370710372925\n",
      "Loss: 0.11896173655986786\n",
      "Loss: 0.11821096390485764\n",
      "Loss: 0.11967425048351288\n",
      "Loss: 0.11533577740192413\n",
      "Loss: 0.11665277928113937\n",
      "Loss: 0.11739557236433029\n",
      "Loss: 0.11501443386077881\n",
      "Loss: 0.1147775799036026\n",
      "Loss: 0.11428260803222656\n",
      "Loss: 0.11254848539829254\n",
      "Loss: 0.113715261220932\n",
      "Loss: 0.11280465126037598\n",
      "Loss: 0.11393760144710541\n",
      "Loss: 0.11458525061607361\n",
      "Loss: 0.11578264087438583\n",
      "Loss: 0.11572099477052689\n",
      "Loss: 0.11406123638153076\n",
      "Loss: 0.11404884606599808\n",
      "Loss: 0.11434106528759003\n",
      "Loss: 0.11396940797567368\n",
      "Loss: 0.11344937980175018\n",
      "Loss: 0.11319771409034729\n",
      "Loss: 0.11335137486457825\n",
      "Loss: 0.11424347758293152\n",
      "Loss: 0.1156626045703888\n",
      "Loss: 0.11512594670057297\n",
      "Loss: 0.11567984521389008\n",
      "Loss: 0.11622537672519684\n",
      "Loss: 0.11501004546880722\n",
      "Loss: 0.11454501003026962\n",
      "Loss: 0.11370807886123657\n",
      "Loss: 0.1135941594839096\n",
      "Loss: 0.11327491700649261\n",
      "Loss: 0.11364608258008957\n",
      "Loss: 0.11319398880004883\n",
      "Loss: 0.11272810399532318\n",
      "Loss: 0.112162746489048\n",
      "Loss: 0.11142830550670624\n",
      "Loss: 0.11077524721622467\n",
      "Loss: 0.1087665930390358\n",
      "Loss: 0.10998781025409698\n",
      "Loss: 0.10870607197284698\n",
      "Loss: 0.10391350090503693\n",
      "Loss: 0.10093298554420471\n",
      "Loss: 0.10699427127838135\n",
      "Loss: 0.10483524203300476\n",
      "Loss: 0.10333415865898132\n",
      "Loss: 0.10382461547851562\n",
      "Loss: 0.11377276480197906\n",
      "Loss: 0.11083578318357468\n",
      "Loss: 0.10697279870510101\n",
      "Loss: 0.10770547389984131\n",
      "Loss: 0.10628851503133774\n",
      "Loss: 0.10648967325687408\n",
      "Loss: 0.1019977480173111\n",
      "Loss: 0.09914946556091309\n",
      "Loss: 0.09663122892379761\n",
      "Loss: 0.09895212203264236\n",
      "Loss: 0.09823109209537506\n",
      "Loss: 0.12105558812618256\n",
      "Loss: 0.15549501776695251\n",
      "Loss: 0.1479579210281372\n",
      "Loss: 0.14049820601940155\n",
      "Epoch: 2\n",
      "Loss: 0.12358882278203964\n",
      "Loss: 0.10655156522989273\n",
      "Loss: 0.11972465366125107\n",
      "Loss: 0.09986230731010437\n",
      "Loss: 0.09873076528310776\n",
      "Loss: 0.09612022340297699\n",
      "Loss: 0.09203429520130157\n",
      "Loss: 0.08399433642625809\n",
      "Loss: 0.09118890762329102\n",
      "Loss: 0.09439685940742493\n",
      "Loss: 0.10037410259246826\n",
      "Loss: 0.09228825569152832\n",
      "Loss: 0.08216975629329681\n",
      "Loss: 0.09726853668689728\n",
      "Loss: 0.1239595115184784\n",
      "Loss: 0.15501871705055237\n",
      "Loss: 0.13747356832027435\n",
      "Loss: 0.10474434494972229\n",
      "Loss: 0.10648198425769806\n",
      "Loss: 0.11294299364089966\n",
      "Loss: 0.10810388624668121\n",
      "Loss: 0.10927097499370575\n",
      "Loss: 0.10687147080898285\n",
      "Loss: 0.10627955198287964\n",
      "Loss: 0.10619957000017166\n",
      "Loss: 0.11873547732830048\n",
      "Loss: 0.1183704137802124\n",
      "Loss: 0.10943901538848877\n",
      "Loss: 0.10825621336698532\n",
      "Loss: 0.10579413175582886\n",
      "Loss: 0.10929249227046967\n",
      "Loss: 0.10782212764024734\n",
      "Loss: 0.10761074721813202\n",
      "Loss: 0.10767869651317596\n",
      "Loss: 0.10238347947597504\n",
      "Loss: 0.10370148718357086\n",
      "Loss: 0.09984342008829117\n",
      "Loss: 0.09551142156124115\n",
      "Loss: 0.09341946244239807\n",
      "Loss: 0.10688736289739609\n",
      "Loss: 0.09732632339000702\n",
      "Loss: 0.1342957615852356\n",
      "Loss: 0.12170733511447906\n",
      "Loss: 0.09169347584247589\n",
      "Loss: 0.07989202439785004\n",
      "Loss: 0.08480756729841232\n",
      "Loss: 0.0980663150548935\n",
      "Loss: 0.10897636413574219\n",
      "Loss: 0.0840248242020607\n",
      "Loss: 0.10156300663948059\n",
      "Loss: 0.12124329805374146\n",
      "Loss: 0.1419532597064972\n",
      "Loss: 0.1213340312242508\n",
      "Loss: 0.1107528880238533\n",
      "Loss: 0.10158748924732208\n",
      "Loss: 0.09590773284435272\n",
      "Loss: 0.11193576455116272\n",
      "Loss: 0.11855553090572357\n",
      "Loss: 0.13520431518554688\n",
      "Loss: 0.1268669217824936\n",
      "Loss: 0.12174303084611893\n",
      "Loss: 0.11412270367145538\n",
      "Loss: 0.1163850799202919\n",
      "Loss: 0.12183641642332077\n",
      "Epoch: 3\n",
      "Loss: 0.11932496726512909\n",
      "Loss: 0.1157894879579544\n",
      "Loss: 0.11518937349319458\n",
      "Loss: 0.11378263682126999\n",
      "Loss: 0.11223740130662918\n",
      "Loss: 0.11115080118179321\n",
      "Loss: 0.11037592589855194\n",
      "Loss: 0.10625861585140228\n",
      "Loss: 0.10363101959228516\n",
      "Loss: 0.09548115730285645\n",
      "Loss: 0.09033621102571487\n",
      "Loss: 0.09264488518238068\n",
      "Loss: 0.09207434952259064\n",
      "Loss: 0.07984858751296997\n",
      "Loss: 0.08895272761583328\n",
      "Loss: 0.07682456076145172\n",
      "Loss: 0.09218741953372955\n",
      "Loss: 0.09173530340194702\n",
      "Loss: 0.0982820987701416\n",
      "Loss: 0.11749482154846191\n",
      "Loss: 0.10163315385580063\n",
      "Loss: 0.1027398630976677\n",
      "Loss: 0.10193029791116714\n",
      "Loss: 0.10444372147321701\n",
      "Loss: 0.09661902487277985\n",
      "Loss: 0.09434723109006882\n",
      "Loss: 0.08952295780181885\n",
      "Loss: 0.0794205367565155\n",
      "Loss: 0.10572220385074615\n",
      "Loss: 0.10223964601755142\n",
      "Loss: 0.06885144114494324\n",
      "Loss: 0.07315289974212646\n",
      "Loss: 0.07636451721191406\n",
      "Loss: 0.09459185600280762\n",
      "Loss: 0.09325969219207764\n",
      "Loss: 0.12938591837882996\n",
      "Loss: 0.0918266549706459\n",
      "Loss: 0.08146572858095169\n",
      "Loss: 0.0937589630484581\n",
      "Loss: 0.09904475510120392\n",
      "Loss: 0.0986037403345108\n",
      "Loss: 0.12434786558151245\n",
      "Loss: 0.09094798564910889\n",
      "Loss: 0.08353649079799652\n",
      "Loss: 0.07529155910015106\n",
      "Loss: 0.07324148714542389\n",
      "Loss: 0.059066787362098694\n",
      "Loss: 0.05794920399785042\n",
      "Loss: 0.05957265943288803\n",
      "Loss: 0.10274017602205276\n",
      "Loss: 0.15218520164489746\n",
      "Loss: 0.16986095905303955\n",
      "Loss: 0.13633568584918976\n",
      "Loss: 0.12061776220798492\n",
      "Loss: 0.10267177224159241\n",
      "Loss: 0.10553106665611267\n",
      "Loss: 0.09517946094274521\n",
      "Loss: 0.10542891174554825\n",
      "Loss: 0.11628688126802444\n",
      "Loss: 0.11316651105880737\n",
      "Loss: 0.12578332424163818\n",
      "Loss: 0.12263911962509155\n",
      "Loss: 0.11633090674877167\n",
      "Loss: 0.11222992837429047\n",
      "Epoch: 4\n",
      "Loss: 0.11834020912647247\n",
      "Loss: 0.11852684617042542\n",
      "Loss: 0.11567030847072601\n",
      "Loss: 0.11480732262134552\n",
      "Loss: 0.11667965352535248\n",
      "Loss: 0.11751189827919006\n",
      "Loss: 0.11770133674144745\n",
      "Loss: 0.11219985783100128\n",
      "Loss: 0.11314111948013306\n",
      "Loss: 0.11043499410152435\n",
      "Loss: 0.10790488123893738\n",
      "Loss: 0.10528570413589478\n",
      "Loss: 0.10149181634187698\n",
      "Loss: 0.10027468204498291\n",
      "Loss: 0.09598386287689209\n",
      "Loss: 0.08966343104839325\n",
      "Loss: 0.08802756667137146\n",
      "Loss: 0.08283967524766922\n",
      "Loss: 0.0881713330745697\n",
      "Loss: 0.12328125536441803\n",
      "Loss: 0.08542386442422867\n",
      "Loss: 0.12154921889305115\n",
      "Loss: 0.09469908475875854\n",
      "Loss: 0.10365025699138641\n",
      "Loss: 0.10491284728050232\n",
      "Loss: 0.10312065482139587\n",
      "Loss: 0.09804366528987885\n",
      "Loss: 0.10287490487098694\n",
      "Loss: 0.12899035215377808\n",
      "Loss: 0.1201276034116745\n",
      "Loss: 0.0952642560005188\n",
      "Loss: 0.08746296167373657\n",
      "Loss: 0.083125039935112\n",
      "Loss: 0.07219231128692627\n",
      "Loss: 0.07784150540828705\n",
      "Loss: 0.09202945232391357\n",
      "Loss: 0.07177843153476715\n",
      "Loss: 0.08029092848300934\n",
      "Loss: 0.14414773881435394\n",
      "Loss: 0.10537330061197281\n",
      "Loss: 0.09140688180923462\n",
      "Loss: 0.07993356883525848\n",
      "Loss: 0.06776337325572968\n",
      "Loss: 0.07339735329151154\n",
      "Loss: 0.07889710366725922\n",
      "Loss: 0.08012974262237549\n",
      "Loss: 0.06672818958759308\n",
      "Loss: 0.07201816886663437\n",
      "Loss: 0.0783710926771164\n",
      "Loss: 0.11097350716590881\n",
      "Loss: 0.09665149450302124\n",
      "Loss: 0.11152953654527664\n",
      "Loss: 0.10109831392765045\n",
      "Loss: 0.102633535861969\n",
      "Loss: 0.12075543403625488\n",
      "Loss: 0.10650065541267395\n",
      "Loss: 0.10714304447174072\n",
      "Loss: 0.10799646377563477\n",
      "Loss: 0.09665613621473312\n",
      "Loss: 0.08653873205184937\n",
      "Loss: 0.10720209777355194\n",
      "Loss: 0.12336453795433044\n",
      "Loss: 0.11405640840530396\n",
      "Loss: 0.10708373785018921\n",
      "Epoch: 5\n",
      "Loss: 0.11825046688318253\n",
      "Loss: 0.11400432884693146\n",
      "Loss: 0.10394220799207687\n",
      "Loss: 0.11531004309654236\n",
      "Loss: 0.11017376184463501\n",
      "Loss: 0.10650432109832764\n",
      "Loss: 0.10396651923656464\n",
      "Loss: 0.10124613344669342\n",
      "Loss: 0.0988064706325531\n",
      "Loss: 0.09881123900413513\n",
      "Loss: 0.10145500302314758\n",
      "Loss: 0.09482885897159576\n",
      "Loss: 0.09232313185930252\n",
      "Loss: 0.09248343110084534\n",
      "Loss: 0.09439738094806671\n",
      "Loss: 0.0961264818906784\n",
      "Loss: 0.0919635146856308\n",
      "Loss: 0.08781963586807251\n",
      "Loss: 0.08346810936927795\n",
      "Loss: 0.09712380915880203\n",
      "Loss: 0.09143170714378357\n",
      "Loss: 0.10219397395849228\n",
      "Loss: 0.0930325910449028\n",
      "Loss: 0.1053239107131958\n",
      "Loss: 0.11219221353530884\n",
      "Loss: 0.09618362784385681\n",
      "Loss: 0.09459196031093597\n",
      "Loss: 0.0944686084985733\n",
      "Loss: 0.10795708000659943\n",
      "Loss: 0.10057073831558228\n",
      "Loss: 0.10025918483734131\n",
      "Loss: 0.11294595897197723\n",
      "Loss: 0.09971267729997635\n",
      "Loss: 0.09750865399837494\n",
      "Loss: 0.09720924496650696\n",
      "Loss: 0.0886770635843277\n",
      "Loss: 0.08694690465927124\n",
      "Loss: 0.08137396723031998\n",
      "Loss: 0.08584994077682495\n",
      "Loss: 0.0771794319152832\n",
      "Loss: 0.07251815497875214\n",
      "Loss: 0.09291799366474152\n",
      "Loss: 0.06341975927352905\n",
      "Loss: 0.06516294926404953\n",
      "Loss: 0.05782654136419296\n",
      "Loss: 0.06472750008106232\n",
      "Loss: 0.06688013672828674\n",
      "Loss: 0.08770930767059326\n",
      "Loss: 0.0469549298286438\n",
      "Loss: 0.037024423480033875\n",
      "Loss: 0.06492295861244202\n",
      "Loss: 0.10708285868167877\n",
      "Loss: 0.10230232030153275\n",
      "Loss: 0.10261760652065277\n",
      "Loss: 0.13229119777679443\n",
      "Loss: 0.1152687668800354\n",
      "Loss: 0.12379255890846252\n",
      "Loss: 0.11715748906135559\n",
      "Loss: 0.10919202864170074\n",
      "Loss: 0.09443994611501694\n",
      "Loss: 0.11173275113105774\n",
      "Loss: 0.11176332831382751\n",
      "Loss: 0.10793419182300568\n",
      "Loss: 0.10374065488576889\n",
      "Epoch: 6\n",
      "Loss: 0.11708533763885498\n",
      "Loss: 0.11880873143672943\n",
      "Loss: 0.11281974613666534\n",
      "Loss: 0.10607312619686127\n",
      "Loss: 0.10266979783773422\n",
      "Loss: 0.09830707311630249\n",
      "Loss: 0.09280461072921753\n",
      "Loss: 0.08935313671827316\n",
      "Loss: 0.09441763162612915\n",
      "Loss: 0.08854623138904572\n",
      "Loss: 0.09168228507041931\n",
      "Loss: 0.09793555736541748\n",
      "Loss: 0.10027412325143814\n",
      "Loss: 0.09660584479570389\n",
      "Loss: 0.10285399109125137\n",
      "Loss: 0.10222393274307251\n",
      "Loss: 0.09685780107975006\n",
      "Loss: 0.09586477279663086\n",
      "Loss: 0.08522725850343704\n",
      "Loss: 0.08911832422018051\n",
      "Loss: 0.08864296227693558\n",
      "Loss: 0.09732644259929657\n",
      "Loss: 0.08400965481996536\n",
      "Loss: 0.09215401113033295\n",
      "Loss: 0.09156837314367294\n",
      "Loss: 0.08475830405950546\n",
      "Loss: 0.07260638475418091\n",
      "Loss: 0.08145882189273834\n",
      "Loss: 0.08137769997119904\n",
      "Loss: 0.0753716230392456\n",
      "Loss: 0.06900255382061005\n",
      "Loss: 0.07122907042503357\n",
      "Loss: 0.07552209496498108\n",
      "Loss: 0.08875656127929688\n",
      "Loss: 0.07847964763641357\n",
      "Loss: 0.07127921283245087\n",
      "Loss: 0.07370235025882721\n",
      "Loss: 0.07121069729328156\n",
      "Loss: 0.0651627704501152\n",
      "Loss: 0.08180374652147293\n",
      "Loss: 0.07299038767814636\n",
      "Loss: 0.08917411416769028\n",
      "Loss: 0.056370627135038376\n",
      "Loss: 0.12261860072612762\n",
      "Loss: 0.10023617744445801\n",
      "Loss: 0.06737852841615677\n",
      "Loss: 0.0651761144399643\n",
      "Loss: 0.0667741596698761\n",
      "Loss: 0.06269402801990509\n",
      "Loss: 0.06528658419847488\n",
      "Loss: 0.06142247095704079\n",
      "Loss: 0.07320038974285126\n",
      "Loss: 0.05917961150407791\n",
      "Loss: 0.051269300282001495\n",
      "Loss: 0.08781056106090546\n",
      "Loss: 0.07530974596738815\n",
      "Loss: 0.15050068497657776\n",
      "Loss: 0.14845535159111023\n",
      "Loss: 0.08588095009326935\n",
      "Loss: 0.06660789251327515\n",
      "Loss: 0.11127160489559174\n",
      "Loss: 0.1023046225309372\n",
      "Loss: 0.09698358178138733\n",
      "Loss: 0.07792079448699951\n",
      "Epoch: 7\n",
      "Loss: 0.11396840959787369\n",
      "Loss: 0.10946434736251831\n",
      "Loss: 0.08946114778518677\n",
      "Loss: 0.08639779686927795\n",
      "Loss: 0.08102858066558838\n",
      "Loss: 0.07837045192718506\n",
      "Loss: 0.0662921592593193\n",
      "Loss: 0.06006655842065811\n",
      "Loss: 0.07620769739151001\n",
      "Loss: 0.06695456057786942\n",
      "Loss: 0.09459900110960007\n",
      "Loss: 0.1131959855556488\n",
      "Loss: 0.09035010635852814\n",
      "Loss: 0.09646843373775482\n",
      "Loss: 0.0797940194606781\n",
      "Loss: 0.07481832057237625\n",
      "Loss: 0.10423725098371506\n",
      "Loss: 0.08774442970752716\n",
      "Loss: 0.0869225263595581\n",
      "Loss: 0.07817307859659195\n",
      "Loss: 0.06259921193122864\n",
      "Loss: 0.09135328978300095\n",
      "Loss: 0.06710760295391083\n",
      "Loss: 0.08942775428295135\n",
      "Loss: 0.07359512150287628\n",
      "Loss: 0.06859879195690155\n",
      "Loss: 0.05578691512346268\n",
      "Loss: 0.0719970241189003\n",
      "Loss: 0.07527734339237213\n",
      "Loss: 0.057405613362789154\n",
      "Loss: 0.05830001085996628\n",
      "Loss: 0.04179114103317261\n",
      "Loss: 0.03778160735964775\n",
      "Loss: 0.03100258857011795\n",
      "Loss: 0.0766092985868454\n",
      "Loss: 0.09776222705841064\n",
      "Loss: 0.07891949266195297\n",
      "Loss: 0.054209932684898376\n",
      "Loss: 0.06515099108219147\n",
      "Loss: 0.06112733110785484\n",
      "Loss: 0.03541218489408493\n",
      "Loss: 0.09001480042934418\n",
      "Loss: 0.06465969234704971\n",
      "Loss: 0.07304510474205017\n",
      "Loss: 0.06895238161087036\n",
      "Loss: 0.06540301442146301\n",
      "Loss: 0.055198002606630325\n",
      "Loss: 0.04585655778646469\n",
      "Loss: 0.0400175079703331\n",
      "Loss: 0.09836935997009277\n",
      "Loss: 0.05502273514866829\n",
      "Loss: 0.05494429171085358\n",
      "Loss: 0.05179916322231293\n",
      "Loss: 0.05350915342569351\n",
      "Loss: 0.06199753284454346\n",
      "Loss: 0.05293937772512436\n",
      "Loss: 0.103988878428936\n",
      "Loss: 0.0806792601943016\n",
      "Loss: 0.07543221116065979\n",
      "Loss: 0.07486869394779205\n",
      "Loss: 0.10999080538749695\n",
      "Loss: 0.09584933519363403\n",
      "Loss: 0.09434819966554642\n",
      "Loss: 0.09305308759212494\n",
      "Epoch: 8\n",
      "Loss: 0.10994552075862885\n",
      "Loss: 0.10826842486858368\n",
      "Loss: 0.08445189893245697\n",
      "Loss: 0.09032587707042694\n",
      "Loss: 0.07930715382099152\n",
      "Loss: 0.07210975885391235\n",
      "Loss: 0.06083689630031586\n",
      "Loss: 0.06184490770101547\n",
      "Loss: 0.06187485158443451\n",
      "Loss: 0.07058893144130707\n",
      "Loss: 0.055068254470825195\n",
      "Loss: 0.1155523955821991\n",
      "Loss: 0.08379661291837692\n",
      "Loss: 0.15641003847122192\n",
      "Loss: 0.12223396450281143\n",
      "Loss: 0.10015296936035156\n",
      "Loss: 0.08748055249452591\n",
      "Loss: 0.07940742373466492\n",
      "Loss: 0.08008284866809845\n",
      "Loss: 0.08296558260917664\n",
      "Loss: 0.08432723581790924\n",
      "Loss: 0.07773259282112122\n",
      "Loss: 0.06330209970474243\n",
      "Loss: 0.06153014302253723\n",
      "Loss: 0.06695227324962616\n",
      "Loss: 0.06821158528327942\n",
      "Loss: 0.0609755702316761\n",
      "Loss: 0.07236593216657639\n",
      "Loss: 0.07153469324111938\n",
      "Loss: 0.07783357799053192\n",
      "Loss: 0.09352631866931915\n",
      "Loss: 0.0687263011932373\n",
      "Loss: 0.0728212296962738\n",
      "Loss: 0.06090845912694931\n",
      "Loss: 0.05697985738515854\n",
      "Loss: 0.055459924042224884\n",
      "Loss: 0.052003830671310425\n",
      "Loss: 0.06137093901634216\n",
      "Loss: 0.09658423811197281\n",
      "Loss: 0.07679848372936249\n",
      "Loss: 0.04094768688082695\n",
      "Loss: 0.05758927762508392\n",
      "Loss: 0.05286034941673279\n",
      "Loss: 0.10836765915155411\n",
      "Loss: 0.09390231966972351\n",
      "Loss: 0.0714377760887146\n",
      "Loss: 0.07392057031393051\n",
      "Loss: 0.0738053098320961\n",
      "Loss: 0.07069578766822815\n",
      "Loss: 0.07777261734008789\n",
      "Loss: 0.07704845070838928\n",
      "Loss: 0.0768638476729393\n",
      "Loss: 0.08747000992298126\n",
      "Loss: 0.08951105922460556\n",
      "Loss: 0.06278036534786224\n",
      "Loss: 0.05797700956463814\n",
      "Loss: 0.0671503096818924\n",
      "Loss: 0.0545806959271431\n",
      "Loss: 0.05425991117954254\n",
      "Loss: 0.05106835439801216\n",
      "Loss: 0.12249667197465897\n",
      "Loss: 0.11527739465236664\n",
      "Loss: 0.12409800291061401\n",
      "Loss: 0.11160632222890854\n",
      "Epoch: 9\n",
      "Loss: 0.09257632493972778\n",
      "Loss: 0.08664477616548538\n",
      "Loss: 0.07949040830135345\n",
      "Loss: 0.0812300592660904\n",
      "Loss: 0.07062923163175583\n",
      "Loss: 0.07121767103672028\n",
      "Loss: 0.08630384504795074\n",
      "Loss: 0.06798221170902252\n",
      "Loss: 0.0695292204618454\n",
      "Loss: 0.055229395627975464\n",
      "Loss: 0.05428845062851906\n",
      "Loss: 0.08978434652090073\n",
      "Loss: 0.06891733407974243\n",
      "Loss: 0.09051642566919327\n",
      "Loss: 0.13284048438072205\n",
      "Loss: 0.11866693198680878\n",
      "Loss: 0.12210208177566528\n",
      "Loss: 0.10758331418037415\n",
      "Loss: 0.09241966903209686\n",
      "Loss: 0.08466564118862152\n",
      "Loss: 0.08530157804489136\n",
      "Loss: 0.06908626854419708\n",
      "Loss: 0.05804689973592758\n",
      "Loss: 0.06255128234624863\n",
      "Loss: 0.06270374357700348\n",
      "Loss: 0.07620581984519958\n",
      "Loss: 0.06018045172095299\n",
      "Loss: 0.0659153088927269\n",
      "Loss: 0.06771107017993927\n",
      "Loss: 0.06411433219909668\n",
      "Loss: 0.06820378452539444\n",
      "Loss: 0.06995626538991928\n",
      "Loss: 0.05794190242886543\n",
      "Loss: 0.06257150322198868\n",
      "Loss: 0.06891236454248428\n",
      "Loss: 0.06465190649032593\n",
      "Loss: 0.06306859105825424\n",
      "Loss: 0.08175849169492722\n",
      "Loss: 0.07816348224878311\n",
      "Loss: 0.07898983359336853\n",
      "Loss: 0.07541126012802124\n",
      "Loss: 0.06485947221517563\n",
      "Loss: 0.06891544908285141\n",
      "Loss: 0.06975304335355759\n",
      "Loss: 0.06709280610084534\n",
      "Loss: 0.061289794743061066\n",
      "Loss: 0.06277257204055786\n",
      "Loss: 0.062354251742362976\n",
      "Loss: 0.05746154487133026\n",
      "Loss: 0.0770530253648758\n",
      "Loss: 0.06867283582687378\n",
      "Loss: 0.06439211964607239\n",
      "Loss: 0.04972975701093674\n",
      "Loss: 0.047981567680835724\n",
      "Loss: 0.0434686541557312\n",
      "Loss: 0.04282790422439575\n",
      "Loss: 0.08655772358179092\n",
      "Loss: 0.07463134825229645\n",
      "Loss: 0.059669893234968185\n",
      "Loss: 0.06279303878545761\n",
      "Loss: 0.0773017406463623\n",
      "Loss: 0.07366415858268738\n",
      "Loss: 0.08266766369342804\n",
      "Loss: 0.07914663106203079\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "   print(\"Epoch:\", epoch)\n",
    "   for idx, batch in enumerate(train_dataloader):\n",
    "        # get the inputs;\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                       labels=labels)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0V6h3aocsS_"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"imdb_tapas.mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmugNHCN20QF"
   },
   "source": [
    "## Inference\n",
    "\n",
    "As SQA is a bit different due to its conversational nature, we need to run every training example of the a batch one by one through the model (sequentially), overwriting the `prev_labels` token types (which were created by the tokenizer) by the answer predicted by the model. It is based on the [following code](https://github.com/google-research/tapas/blob/f458b6624b8aa75961a0ab78e9847355022940d3/tapas/experiments/prediction_utils.py#L92) from the official implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxE2yxtNUG5j"
   },
   "outputs": [],
   "source": [
    "from transformers import TapasForQuestionAnswering\n",
    "\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"imdb_tapas.mdl\")\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6W1UUCkYZjq"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"queries_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTf9BQgIVlot"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "preds_tapas = {}\n",
    "\n",
    "for i in tqdm(range(0,len(test)),position=0):\n",
    "  item = test.iloc[i]\n",
    "  queries = [item['question']]\n",
    "  table = pd.read_csv(item['table_file']).astype(str)\n",
    "  inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"pt\")\n",
    "  outputs = model(**inputs)\n",
    "  predicted_answer_coordinates = tokenizer.convert_logits_to_predictions(\n",
    "          inputs,\n",
    "          outputs.logits.detach()\n",
    "  )\n",
    "  preds_tapas[item['question']] = [table.iloc[p[0]]['info'] for p in predicted_answer_coordinates[0][0]]\n",
    "  print(predicted_answer_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_GkQS2wvW_e"
   },
   "outputs": [],
   "source": [
    "########################## METRICS\n",
    "\n",
    "def HAS_POSITIVE(actual,preds):\n",
    "    for i in range(0,len(preds)):\n",
    "        if preds[i] in actual:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def MRRR(actual,preds):\n",
    "    for i in range(0,len(preds)):\n",
    "        if preds[i] in actual:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def MAP_K(actual,preds):\n",
    "    precision = 0\n",
    "    hit = 0\n",
    "    for i in range(0,len(preds)):\n",
    "        if preds[i] in actual:\n",
    "            hit += 1\n",
    "            precision += hit/(i+1)\n",
    "    return precision/len(actual)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3407,
     "status": "ok",
     "timestamp": 1634828534799,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "KpsgWgQO4DSF",
    "outputId": "9eb34e8b-347c-48d0-b054-c6e877081c86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42206/42206 [00:02<00:00, 14918.84it/s]\n"
     ]
    }
   ],
   "source": [
    "id_reviews = {v: k for k, v in review_ids.items()}\n",
    "\n",
    "preds_movie = {}\n",
    "\n",
    "for movie in tqdm(movies_dic):\n",
    "  for n in movies_dic[movie]: \n",
    "    r = ' '.join([str(mm) for mm in n if mm not in ['',' ']])\n",
    "    \n",
    "    temp = []\n",
    "\n",
    "    for p in preds_tapas:\n",
    "        rev = id_reviews[p]\n",
    "        if r in preds_tapas[p]:\n",
    "          temp.append(rev) \n",
    "    preds_movie[movie] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1634828535026,
     "user": {
      "displayName": "Naser Ahmadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05278100254213815910"
     },
     "user_tz": -120
    },
    "id": "7HKsX0tEtLyS",
    "outputId": "787295a4-6505-4591-e827-ec898daab406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################### 1 ###########################\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a41dca730d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n#################### '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ###########################\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MRR:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMRR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MAP:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAP\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HAS POSITIVE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasP\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "id_reviews = {v: k for k, v in review_ids.items()}\n",
    "for KK in [1,5,20]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for claim in preds_movie:\n",
    "        if len(preds_movie[claim]) == 0 or claim not in ground_truth: continue\n",
    "        i+=1\n",
    "        preds =  preds_movie[claim] [0:KK]\n",
    "        golds = [f for f in ground_truth[claim]]\n",
    "\n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MRR += MRRR(preds,golds)\n",
    "        hasP += HAS_POSITIVE(preds,golds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TAPAS_IMDB.ipynb",
   "provenance": [
    {
     "file_id": "1hayc6PJKIv5U0PWXLGKYTC2GvMHXc52b",
     "timestamp": 1630834855747
    },
    {
     "file_id": "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb",
     "timestamp": 1630667511750
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

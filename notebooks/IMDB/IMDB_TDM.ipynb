{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:42:04.340218Z",
     "start_time": "2021-07-23T10:42:04.324111Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "pd.options.display.max_colwidth=500\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:42:16.477023Z",
     "start_time": "2021-07-23T10:42:14.064581Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from graphUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:42:38.287026Z",
     "start_time": "2021-07-23T10:42:38.261446Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load(open('../../data/imdb/imdb_reviews_1000film.df','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:42:45.837912Z",
     "start_time": "2021-07-23T10:42:45.294583Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "movies_dic = {}\n",
    "with open('../../data/imdb/imdb_movielens.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        if row[12].replace('_',' ') not in movies_dic: \n",
    "            movies_dic[row[12].replace('_',' ')] = []\n",
    "            \n",
    "        temp = [r.replace('_',' ') for r in row[0:10]]\n",
    "        \n",
    "        month,year = '',''\n",
    "        if len(row[10]) > 0:        \n",
    "            month = datetime.date(1900, int(row[10][4::]), 1).strftime('%B')\n",
    "            year = row[10][0:4]\n",
    "        \n",
    "        temp.append(month.lower() + ' ' + year)\n",
    "        temp.append(int(float(row[14])))\n",
    "        \n",
    "        movies_dic[row[12].replace('_',' ')].append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:43:18.468907Z",
     "start_time": "2021-07-23T10:43:18.462704Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:46:10.035058Z",
     "start_time": "2021-07-23T10:44:08.477736Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for movie in tqdm(movies_dic):\n",
    "    i+=1\n",
    "    row_name = str('RW'+str(i))\n",
    "    G.add_node(row_name , label= row_name, type='Row')\n",
    "    row_ids[row_name] = movie\n",
    "    id_rows[movie] = row_name\n",
    "    j=0\n",
    "    \n",
    "    #cols = [movie]\n",
    "    cols = []\n",
    "    for c in movies_dic[movie]: \n",
    "        for cl in c:\n",
    "            cols.append(cl)\n",
    "    \n",
    "    for cl in cols:\n",
    "        j+=1\n",
    "        col_name = str('CL'+str(j))\n",
    "        if cl == '': continue\n",
    "        \n",
    "        if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "        n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(normalize_text(cl)),K)]\n",
    "        \n",
    "        \n",
    "        for tg in n_grams:\n",
    "            \n",
    "            if not G.has_node(tg):\n",
    "                G.add_node(tg,label=tg, type='Token')\n",
    "            G.add_edge(row_name,tg)\n",
    "            G.add_edge(col_name,tg)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:46:16.863892Z",
     "start_time": "2021-07-23T10:46:10.036590Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "review_ids = {}\n",
    "id_review = {}\n",
    "\n",
    "for row in tqdm(df.itertuples()):\n",
    "    if row.movie.lower() not in movies_dic: continue\n",
    "    i += 1\n",
    "    text = remove_stopwords(normalize_text(row.user_review.lower()))\n",
    "    review_name = str('Review'+str(i))\n",
    "    G.add_node(review_name , label= review_name, type='Review')\n",
    "    review_ids[review_name] = row.user_review\n",
    "    id_review[text] = review_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "\n",
    "    for tg in n_grams:\n",
    "        \n",
    "        \n",
    "        if not G.has_node(tg):\n",
    "            continue\n",
    "           # G.add_node(tg,label=tg, type='Token')\n",
    "            \n",
    "        if not G.has_edge(review_name,tg):            G.add_edge(review_name,tg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:46:17.862981Z",
     "start_time": "2021-07-23T10:46:16.865668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "for row in tqdm(df.itertuples()):\n",
    "    if row.movie.lower() not in movies_dic:         continue\n",
    "    movie_name = row.movie.lower() \n",
    "    if movie_name not in ground_truth: ground_truth[movie_name] = []\n",
    "    #if remove_stopwords(row.user_review.lower()) not in id_review: continue\n",
    "    ground_truth[movie_name]. append(id_review[remove_stopwords(normalize_text(row.user_review.lower()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion with DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T13:10:20.736280Z",
     "start_time": "2021-07-02T13:10:20.722422Z"
    }
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "Q = \"\"\"\n",
    "    select distinct ?o    where {    \n",
    "    <http://dbpedia.org/resource/%s> <http://dbpedia.org/ontology/wikiPageWikiLink> ?o . }\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def run_dbpedia(name):\n",
    "    cname = ''\n",
    "    for n in name.split('_'): cname += n.capitalize() + '_'\n",
    "        \n",
    "    cname = cname[0:-1]\n",
    "    query = Q%cname\n",
    "    \n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    res = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        if not '.jpg' in result[\"o\"][\"value\"] and not 'http://dbpedia.org/resource/Category:' in result[\"o\"][\"value\"]:  \n",
    "            res.append (result[\"o\"][\"value\"].split('/')[-1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T11:12:07.691063Z",
     "start_time": "2021-07-02T13:10:20.738211Z"
    }
   },
   "outputs": [],
   "source": [
    "for node in tqdm(G.copy().nodes()):\n",
    "    if G.nodes()[node]['type'] != 'Token': continue\n",
    "    \n",
    "    if len(node.split('_')) < 2: continue \n",
    "    \n",
    "    if len(node.split('_'))==2 and node.split('_')[1].isdigit(): continue\n",
    "    \n",
    "    try:\n",
    "        for e in run_dbpedia(node):\n",
    "            for n in utils.normalize_text(e).split('_'):\n",
    "                if not G.has_node(n):\n",
    "                    G.add_node(n, label = n, type = 'Token')\n",
    "                    G.add_edge(node,n,type= 'dbp')\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "for n in G.copy().nodes():\n",
    "    if G.degree()[n] < 2:\n",
    "        G.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T20:21:37.517453Z",
     "start_time": "2021-07-04T20:21:29.268434Z"
    }
   },
   "outputs": [],
   "source": [
    "#nx.write_graphml(G,'data/imdb/imdb_expanded.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSuM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T10:58:35.313545Z",
     "start_time": "2021-06-09T10:58:35.174437Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "node_ids = {}\n",
    "\n",
    "for n in G.nodes:\n",
    "    node_ids[n] = i\n",
    "    i+=1\n",
    "inv_nodes = {v: k for k, v in node_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T11:08:08.183249Z",
     "start_time": "2021-06-09T11:08:04.745210Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('imdb_edgelist', 'w')\n",
    "\n",
    "for e in G.edges():    file.write(str(node_ids[e[0]]) + '\\t' + str(node_ids[e[1]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T09:10:13.367711Z",
     "start_time": "2021-06-10T09:10:13.006819Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../SSumM/output/summary_imdb_edgelist_0.9.txt') as f:\n",
    "    sum_grapph = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "sum_grapph = [x.strip() for x in sum_grapph] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T09:10:16.975795Z",
     "start_time": "2021-06-10T09:10:13.879581Z"
    }
   },
   "outputs": [],
   "source": [
    "super_nodes,super_edges = {},[]\n",
    "edge_weights = {}\n",
    "\n",
    "for i in range(1,sum_grapph.index('<Superedge info>')):\n",
    "    node = sum_grapph[i].split('\\t')\n",
    "    idd = node[0]\n",
    "    node = [inv_nodes[int(n)] for n in node[1::]]\n",
    "    super_nodes[idd] = node\n",
    "\n",
    "for i in range(sum_grapph.index('<Superedge info>')+1,len(sum_grapph)):\n",
    "    e = sum_grapph[i].split('\\t')\n",
    "    if e[0] not in edge_weights:        edge_weights[e[0]] = {}\n",
    "    if e[1] not in edge_weights:        edge_weights[e[1]] = {}\n",
    "        \n",
    "\n",
    "    edge_weights[e[0]][e[1]] = e[2]\n",
    "    edge_weights[e[1]][e[0]] = e[2]\n",
    "    \n",
    "    super_edges.append((e[0],e[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T09:10:27.060220Z",
     "start_time": "2021-06-10T09:10:17.945682Z"
    }
   },
   "outputs": [],
   "source": [
    "SG = nx.Graph()\n",
    "\n",
    "for node in super_nodes:\n",
    "    name = ''\n",
    "    if ' '.join(super_nodes[node]).startswith(('RW','Review','CL')):\n",
    "        name = ' '.join(super_nodes[node])\n",
    "    else:\n",
    "        name = super_nodes[node][0]\n",
    "        \n",
    "    SG.add_node(node , label= name, type='node')\n",
    "    \n",
    "for e in super_edges:\n",
    "    SG.add_edge(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T09:10:28.578214Z",
     "start_time": "2021-06-10T09:10:28.504257Z"
    }
   },
   "outputs": [],
   "source": [
    "G = SG\n",
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T12:15:22.463854Z",
     "start_time": "2021-07-20T21:57:58.328524Z"
    }
   },
   "outputs": [],
   "source": [
    "#G1 = nx.Graph()\n",
    "from random import choice\n",
    "L = int(len(G.nodes())/2)\n",
    "sp = []\n",
    "i =0 \n",
    "pbar = tqdm(total=L,position=0)\n",
    "while i < L:\n",
    "    first = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Row'])\n",
    "    second = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Review'])\n",
    "    paths = nx.all_shortest_paths(G, first,second, weight=None)\n",
    "    for p in paths:\n",
    "        G1.add_nodes_from(p)\n",
    "        nx.add_path(G1,p)    \n",
    "    i+=1\n",
    "    pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T12:15:24.747318Z",
     "start_time": "2021-07-21T12:15:24.690392Z"
    }
   },
   "outputs": [],
   "source": [
    "G = G1\n",
    "len(G.nodes()), len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-23T10:45:24.581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(G,30,l=30)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T13:14:42.819347Z",
     "start_time": "2021-07-21T13:14:42.815103Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T13:24:13.373921Z",
     "start_time": "2021-07-21T13:14:42.821770Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T13:46:00.317332Z",
     "start_time": "2021-07-21T13:24:13.375935Z"
    }
   },
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T09:25:45.814399Z",
     "start_time": "2021-07-23T09:25:45.804038Z"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T13:46:22.829024Z",
     "start_time": "2021-07-21T13:46:00.318983Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_reviews = {}\n",
    "for movie in tqdm(ground_truth):\n",
    "    review_new = []\n",
    "    \n",
    "    m_id = id_rows[movie]\n",
    "    \n",
    "    for r in ground_truth[movie]:\n",
    "        review_new.append(r)\n",
    "        \n",
    "    for r in random.sample(review_ids.keys(),98):\n",
    "        if r not in ground_truth[movie]:\n",
    "            review_new.append(r)\n",
    "    \n",
    "    \n",
    "    \n",
    "    movie_reviews[movie] = distance_w2v (model,m_id,review_ids,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T13:46:23.312481Z",
     "start_time": "2021-07-21T13:46:22.830449Z"
    }
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,50000]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for movie in movie_reviews:\n",
    "        if movie not in ground_truth: continue\n",
    "        i+=1\n",
    "        preds = [f for (f,j) in movie_reviews[movie]][0:KK]\n",
    "        golds = [f for f in ground_truth[movie]]\n",
    "\n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from imdbUtils import *\n",
    "import csv\n",
    "pd.options.display.max_colwidth=500\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corona Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "all_tables = []\n",
    "\n",
    "directory = 'data/Corona/tables/'\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        with open(directory + file) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=',')\n",
    "            \n",
    "            next(spamreader)\n",
    "            for row in spamreader:\n",
    "                temp = [file.split('.')[0].replace('_',' ')]\n",
    "                temp.append(row[0].lower())\n",
    "                for r in row[-6::]: temp.append(str(int(float(r))))\n",
    "                all_tables.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = {1:'table',2:'country',3:'january',4:'february',5:'march',6:'april',7:'may',8:'june'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claims = {}\n",
    "\n",
    "with open('data/Corona/Claims_English.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for r in csv_reader:\n",
    "        if r[-1] == 'a':\n",
    "            all_claims[r[0].lower()] = [r[1].replace('_',' '),r[2].lower(),r[3].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('corona_GEN.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for claim in all_claims:\n",
    "        temp = [claim]\n",
    "        for c in all_claims[claim][0:4]: temp.append(c)\n",
    "        spamwriter.writerow(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_claims = {}\n",
    "\n",
    "with open('data/Corona/Claims_users.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for r in csv_reader:\n",
    "        user_claims[r[0].lower()] = []        \n",
    "        for rr in r[1].split('-'):\n",
    "            for rrr in r[2].split('-'):\n",
    "                temp = [rr.lower(),rrr.replace('_',' ').lower()]\n",
    "                for rrrr in r[3::]:\n",
    "                    temp.append(rrrr.lower().replace('_',' '))\n",
    "            user_claims[r[0].lower()].append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import graphUtils\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "def return_n_grams(text,k):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = set()\n",
    "    for i in range(0,len(tokens)-(k-1)):\n",
    "        n_grams.add( ' '.join( ( [tk for tk in tokens[i:i+k]]) ))\n",
    "        \n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def find_all_n_grams (text,n):\n",
    "    n_grams = []\n",
    "    for k in range(1,n+1):\n",
    "        k_grams = return_n_grams(text,k)\n",
    "        for g in k_grams: n_grams.append(g)\n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 2\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for row in tqdm(all_tables):\n",
    "    i+=1\n",
    "    row_name = str('RW'+str(i))\n",
    "    G.add_node(row_name , label= row_name, type='Row')\n",
    "    row_ids[row_name] = ' '.join([r for r in row])\n",
    "    id_rows[' '.join(row)] = row_name\n",
    "    \n",
    "    j=0\n",
    "    for cl in row:\n",
    "        j+=1\n",
    "        col_name = table_columns[j]\n",
    "                \n",
    "        if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "        n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(cl),K)]\n",
    "        \n",
    "        for tg in n_grams:\n",
    "            if not G.has_node(tg): G.add_node(tg,label=tg, type='Token')\n",
    "            G.add_edge(row_name,tg)\n",
    "            G.add_edge(col_name,tg)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "all_claims.update(user_claims)\n",
    "\n",
    "for claim in tqdm(all_claims):\n",
    "    i += 1\n",
    "    text = remove_stopwords(claim)\n",
    "    claim_name = str('Claim'+str(i))\n",
    "    G.add_node(claim_name , label= claim_name, type='Claim')\n",
    "    claim_ids[claim_name] = claim\n",
    "    id_claim[claim] = claim_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "            \n",
    "        if not G.has_node(token): continue\n",
    "\n",
    "        if not G.has_edge(claim_name,token):            G.add_edge(claim_name,token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "for cl in claim_ids:\n",
    "    ground_truth[cl] = []\n",
    "    if claim_ids[cl] in user_claims:\n",
    "        for r in user_claims[claim_ids[cl]]:\n",
    "            for rr in id_rows:\n",
    "                if ' '.join(reversed(r)) in rr:             \n",
    "                    ground_truth[cl].append(id_rows[rr])\n",
    "    else:\n",
    "        for r in id_rows:\n",
    "            if ' '.join(all_claims[claim_ids[cl]][0:2]) in r:\n",
    "                ground_truth[cl].append(id_rows[r])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(20,l=20)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 30\n",
    "vec_size = 300\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    if claim in model.wv:\n",
    "        movie_reviews[claim] = utils.distance_w2v (model,claim,row_ids,len(row_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for KK in [1,2,3,5,10,20,50]: \n",
    "#for KK in [2]: \n",
    "    \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for movie in movie_reviews:\n",
    "        if movie not in ground_truth or len(ground_truth[movie])==0: continue\n",
    "        #if movie in [id_claim[c] for c in user_claims]: continue\n",
    "            \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in movie_reviews[movie]][0:KK]\n",
    "        golds = [g for g in ground_truth[movie]]\n",
    "        \n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MRR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "ps = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_info = {}\n",
    "facts = []\n",
    "with open('data/KnownLie/politifact/verified-claims/facts_with_articles.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        facts.append(row[1])\n",
    "        facts_info[row[1]] = remove_stopwords(utils.normalize_text(' '.join([row[1],row[6]])))\n",
    "        #facts_info[row[1]] = remove_stopwords(utils.normalize_text(row[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from gensim.summarization.bm25 import BM25\n",
    "corpus = [f.split(\" \") for f in facts]\n",
    "results = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "all_claims = []\n",
    "\n",
    "directory = 'data/KnownLie/politifact/fact-checking/'\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        with open(directory + file) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=',')\n",
    "            \n",
    "            next(spamreader)\n",
    "            for row in spamreader:\n",
    "                text = row[4]\n",
    "                scores = results.get_scores(text.split())\n",
    "                idx = scores.index(max(scores))\n",
    "                text = ' '.join(corpus[idx])\n",
    "                \n",
    "                if row[3] == 'Carbon copies': title = row[2]\n",
    "                else:  title = row[3]\n",
    "                title = title\n",
    "                if title not in ground_truth:\n",
    "                    ground_truth[title] = []\n",
    "                all_claims.append(text)\n",
    "                ground_truth[title].append((text,row[5]))\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(ground_truth,open('politi_GT.pkl','wb'))\n",
    "pickle.dump(facts,open('politi_all_facts.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import graphUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia2vec import Wikipedia2Vec\n",
    "\n",
    "wvmodel = Wikipedia2Vec.load('models/pretrained/enwiki_20180420_300d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 2\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "\n",
    "for claim in tqdm([g for g in ground_truth.keys()]):\n",
    "    node = remove_stopwords(utils.normalize_text(claim))\n",
    "    i+=1\n",
    "\n",
    "    node_name = str('CLM'+str(i))\n",
    "    G.add_node(node_name , label= node_name, type='Claim')\n",
    "    nodes_labels[node_name] = node_name\n",
    "    claim_ids[node_name] = claim\n",
    "    id_claim[claim] = node_name\n",
    "                \n",
    "        \n",
    "    n_grams = [gr.replace(' ','_') for gr in graphUtils.find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        #if len(tg.split('_')) == 1: tg = utils.ps.stem(tg)\n",
    "        token = tg\n",
    "        \n",
    "            \n",
    "        G.add_node(token,label=token, type='Token')\n",
    "            \n",
    "        if not G.has_edge(node_name,token): G.add_edge(node_name,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fact_ids = {}\n",
    "id_fact = {}\n",
    "node_maps = []\n",
    "\n",
    "for fact in tqdm(facts):\n",
    "    node = remove_stopwords(utils.normalize_text(facts_info[fact]))\n",
    "    i += 1\n",
    "    name = str('FCT'+ str(i))\n",
    "    \n",
    "    fact_ids[name] = fact\n",
    "    id_fact[fact] = name\n",
    "    \n",
    "    G.add_node(name,label = name, type='Fact')\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in graphUtils.find_all_n_grams(node,K)]\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        #if len(tg.split('_')) == 1: tg = utils.ps.stem(tg)\n",
    "        token = tg\n",
    "        if not G.has_node(token): continue\n",
    "\n",
    "        if not G.has_edge(name,token):            G.add_edge(name,token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(20,l=20)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=20, sg=1, seed=0, workers = 4)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_facts = {}\n",
    "for claim in ground_truth:\n",
    "    if claim not in gold_facts:\n",
    "        gold_facts[claim] = []\n",
    "    for fact in ground_truth[claim]:\n",
    "        if fact[0] in id_fact:\n",
    "            gold_facts[claim].append(id_fact[fact[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_facts = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    if claim not in id_claim: continue\n",
    "    cl_id = id_claim[claim]\n",
    "    filtered_facts = {}\n",
    "    \n",
    "    if cl_id not in model.wv: continue\n",
    "    claim_facts[cl_id] = utils.distance_w2v (model,cl_id,fact_ids,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for KK in [1,3,5,10,20,50]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for claim in claim_facts:\n",
    "        if claim_ids[claim] not in gold_facts or len(gold_facts[claim_ids[claim]]) == 0: continue\n",
    "        \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in claim_facts[claim]][0:KK]\n",
    "        golds = [f for f in gold_facts[claim_ids[claim]]]\n",
    "\n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MRR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "\n",
    "\n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

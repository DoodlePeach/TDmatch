{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:23:22.558888Z",
     "start_time": "2021-07-23T10:23:22.093888Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "pd.options.display.max_colwidth=500\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:23:25.937679Z",
     "start_time": "2021-07-23T10:23:23.461571Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from graphUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corona Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:26:17.102638Z",
     "start_time": "2021-07-23T10:26:17.084975Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "all_tables = []\n",
    "\n",
    "directory = '../../data/Corona/tables/'\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        with open(directory + file) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=',')\n",
    "            \n",
    "            next(spamreader)\n",
    "            for row in spamreader:\n",
    "                temp = [file.split('.')[0].replace('_',' ')]\n",
    "                temp.append(row[0].lower())\n",
    "                for r in row[-6::]: temp.append(str(int(float(r))))\n",
    "                all_tables.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:23:28.669730Z",
     "start_time": "2021-07-23T10:23:28.666432Z"
    }
   },
   "outputs": [],
   "source": [
    "table_columns = {1:'table',2:'country',3:'january',4:'february',5:'march',6:'april',7:'may',8:'june'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:27:49.165696Z",
     "start_time": "2021-07-23T10:27:49.137484Z"
    }
   },
   "outputs": [],
   "source": [
    "all_claims = pickle.load(open('../../data/corona/corona_allClaims','rb'))\n",
    "user_claims = pickle.load(open('../../data/corona/user_claims.pkl','rb'))\n",
    "all_tables = pickle.load(open('../../data/corona/corona_tables','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:27:51.964290Z",
     "start_time": "2021-07-23T10:27:50.140046Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "f,k =0,0\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for row in tqdm(all_tables):\n",
    "    i+=1\n",
    "    row_name = str('RW'+str(i))\n",
    "    G.add_node(row_name , label= row_name, type='Row')\n",
    "    row_ids[row_name] = ' '.join([r for r in row])\n",
    "    id_rows[' '.join(row)] = row_name\n",
    "    \n",
    "    j=0\n",
    "    for cl in row:\n",
    "        j+=1\n",
    "        col_name = table_columns[j]\n",
    "                \n",
    "        if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "        n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(cl),K)]\n",
    "        \n",
    "        for tg in n_grams:\n",
    "            if not G.has_node(tg): G.add_node(tg,label=tg, type='Token')\n",
    "            G.add_edge(row_name,tg)\n",
    "            G.add_edge(col_name,tg)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:27:54.747618Z",
     "start_time": "2021-07-23T10:27:52.402085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "all_claims.update(user_claims)\n",
    "new_nodes = 0\n",
    "\n",
    "for claim in tqdm(all_claims):\n",
    "    i += 1\n",
    "    text = ' '.join([w for w in normalize_text(claim).split() if w not in stop_words])\n",
    "    claim_name = str('Claim'+str(i))\n",
    "    G.add_node(claim_name , label= claim_name, type='Claim')\n",
    "    claim_ids[claim_name] = claim\n",
    "    id_claim[claim] = claim_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "        \n",
    "        if not G.has_node(token): continue\n",
    "        if not G.has_edge(claim_name,token):            G.add_edge(claim_name,token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:27:57.722713Z",
     "start_time": "2021-07-23T10:27:55.272286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "for cl in claim_ids:\n",
    "    ground_truth[cl] = []\n",
    "    if claim_ids[cl] in user_claims:\n",
    "        for r in user_claims[claim_ids[cl]]:\n",
    "            for rr in id_rows:\n",
    "                if ' '.join(reversed(r)) in rr:             \n",
    "                    ground_truth[cl].append(id_rows[rr])\n",
    "    else:\n",
    "        for r in id_rows:\n",
    "            if ' '.join(all_claims[claim_ids[cl]][0:2]) in r:\n",
    "                ground_truth[cl].append(id_rows[r])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:27:58.681140Z",
     "start_time": "2021-07-23T10:27:58.662793Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion with ConceptNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-16T15:31:16.232Z"
    }
   },
   "outputs": [],
   "source": [
    "import conceptnet_lite\n",
    "conceptnet_lite.connect(\"../MatchingText/conceptnet.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-16T15:31:16.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from conceptnet_lite import Label, edges_for\n",
    "from tqdm import tqdm\n",
    "\n",
    "new_nodes = []\n",
    "\n",
    "for node in tqdm(G.copy().nodes(),position=0):\n",
    "    if G.nodes()[node]['type'] != 'Token': continue\n",
    "    \n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=G.nodes()[node]['label'].replace('_',' '), language='en').concepts, same_language=True):\n",
    "            if e.start.text == node:\n",
    "                new_node = e.end.text\n",
    "            else:\n",
    "                new_node = e.start.text\n",
    "            rel = e.relation.name\n",
    "            \n",
    "            for n in utils.normalize_text(new_node).split():\n",
    "                if not G.has_node(n):\n",
    "                    new_nodes.append(n)\n",
    "                    G.add_node(n, label = n, type = 'Token')\n",
    "            G.add_edge(node,n,type= rel)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "for n in G.copy().nodes():\n",
    "    if G.degree()[n] < 2:\n",
    "        G.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-16T15:31:16.239Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-16T15:31:16.246Z"
    }
   },
   "outputs": [],
   "source": [
    "#nx.write_graphml(G,'../MatchingText/data/Corona/corona_expanded.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSuM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:35:01.960525Z",
     "start_time": "2021-07-20T17:35:01.951951Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "node_ids = {}\n",
    "\n",
    "for n in G.nodes:\n",
    "    node_ids[n] = i\n",
    "    i+=1\n",
    "inv_nodes = {v: k for k, v in node_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:35:04.199815Z",
     "start_time": "2021-07-20T17:35:04.124809Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('corona_edgelist', 'w')\n",
    "\n",
    "for e in G.edges():    file.write(str(node_ids[e[0]]) + '\\t' + str(node_ids[e[1]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:40:45.063285Z",
     "start_time": "2021-07-20T17:40:45.052547Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../SSumM/output/summary_corona_edgelist.txt') as f:\n",
    "    sum_grapph = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "sum_grapph = [x.strip() for x in sum_grapph] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:40:46.405267Z",
     "start_time": "2021-07-20T17:40:46.330067Z"
    }
   },
   "outputs": [],
   "source": [
    "super_nodes,super_edges = {},[]\n",
    "edge_weights = {}\n",
    "\n",
    "for i in range(1,sum_grapph.index('<Superedge info>')):\n",
    "    node = sum_grapph[i].split('\\t')\n",
    "    idd = node[0]\n",
    "    node = [inv_nodes[int(n)] for n in node[1::]]\n",
    "    super_nodes[idd] = node\n",
    "\n",
    "for i in range(sum_grapph.index('<Superedge info>')+1,len(sum_grapph)):\n",
    "    e = sum_grapph[i].split('\\t')\n",
    "    if e[0] not in edge_weights:        edge_weights[e[0]] = {}\n",
    "    if e[1] not in edge_weights:        edge_weights[e[1]] = {}\n",
    "        \n",
    "\n",
    "    edge_weights[e[0]][e[1]] = e[2]\n",
    "    edge_weights[e[1]][e[0]] = e[2]\n",
    "    \n",
    "    super_edges.append((e[0],e[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:40:47.438696Z",
     "start_time": "2021-07-20T17:40:47.348483Z"
    }
   },
   "outputs": [],
   "source": [
    "SG = nx.Graph()\n",
    "\n",
    "for node in super_nodes:\n",
    "    name = ''\n",
    "    if ' '.join(super_nodes[node]).startswith(('RW','Review','CL')):\n",
    "        name = ' '.join(super_nodes[node])\n",
    "    else:\n",
    "        name = super_nodes[node][0]\n",
    "        \n",
    "    SG.add_node(node , label= name, type='node')\n",
    "    \n",
    "for e in super_edges:\n",
    "    SG.add_edge(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:40:48.615231Z",
     "start_time": "2021-07-20T17:40:48.609778Z"
    }
   },
   "outputs": [],
   "source": [
    "G = SG\n",
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:56:59.700338Z",
     "start_time": "2021-07-20T15:50:05.430965Z"
    }
   },
   "outputs": [],
   "source": [
    "G1 = nx.Graph()\n",
    "\n",
    "from random import choice\n",
    "L = int(len(G.nodes())/4)\n",
    "sp = []\n",
    "i =0 \n",
    "pbar = tqdm(total=L,position=0)\n",
    "while i < L:\n",
    "    first = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Row'])\n",
    "    second = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Claim'])\n",
    "    paths = [p for p in nx.all_shortest_paths(G, first,second, weight=None)]\n",
    "    for p in paths:\n",
    "        G1.add_nodes_from(p)\n",
    "        nx.add_path(G1,p)    \n",
    "    i+=1\n",
    "    pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:56:59.721934Z",
     "start_time": "2021-07-20T15:56:59.702383Z"
    }
   },
   "outputs": [],
   "source": [
    "G = G1\n",
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:30:22.435656Z",
     "start_time": "2021-07-23T10:28:07.919588Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(G,100,l=25)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:49:12.656452Z",
     "start_time": "2021-07-20T17:49:12.652515Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:52:24.223536Z",
     "start_time": "2021-07-20T17:49:12.659239Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:58:44.387218Z",
     "start_time": "2021-07-20T17:52:24.225012Z"
    }
   },
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T18:08:02.625754Z",
     "start_time": "2021-07-20T18:08:02.615386Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_reviews = {}\n",
    "for claim in tqdm(ground_truth,position=0):\n",
    "    if claim in model.wv:\n",
    "        movie_reviews[claim] = distance_w2v (model,claim,row_ids,len(row_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T18:08:03.771918Z",
     "start_time": "2021-07-20T18:08:03.764878Z"
    }
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,len(row_ids)]: \n",
    "#for KK in [2]: \n",
    "    \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for movie in movie_reviews:\n",
    "        if movie not in ground_truth or len(ground_truth[movie])==0: continue\n",
    "        if movie in [id_claim[c] for c in user_claims]: continue\n",
    "            \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in movie_reviews[movie]][0:KK]\n",
    "        golds = [g for g in ground_truth[movie]]\n",
    "        \n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    try:\n",
    "        print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)\n",
    "    except:\n",
    "        print('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:18:14.568305Z",
     "start_time": "2021-07-20T16:18:14.323926Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,500]: \n",
    "#for KK in [2]: \n",
    "    \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for movie in movie_reviews:\n",
    "        if movie not in ground_truth or len(ground_truth[movie])==0: continue\n",
    "        if movie not in [id_claim[c] for c in user_claims]: continue\n",
    "            \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in movie_reviews[movie]][0:KK]\n",
    "        golds = [g for g in ground_truth[movie]]\n",
    "        \n",
    "        \n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    try:\n",
    "        print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)\n",
    "    except:\n",
    "        print('y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:31:09.611788Z",
     "start_time": "2021-07-23T10:31:08.628723Z"
    },
    "id": "fvFvBLJV0Dkv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:32:05.977708Z",
     "start_time": "2021-07-23T10:32:03.570982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from graphUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:31:38.511507Z",
     "start_time": "2021-07-23T10:31:38.486708Z"
    },
    "id": "cyoj29J24hPX"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_claims = pickle.load(open('../../data/corona/corona_allClaims','rb'))\n",
    "user_claims = pickle.load(open('../../data/corona/user_claims.pkl','rb'))\n",
    "all_tables = pickle.load(open('../../data/corona/corona_tables','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:32:11.009299Z",
     "start_time": "2021-07-23T10:32:06.700600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQGeDwxYMfsI",
    "outputId": "5b84ad6c-02db-447d-d9b9-98e20b782f88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1158/1158 [00:01<00:00, 702.94it/s]\n",
      "100%|██████████| 7058/7058 [00:01<00:00, 4096.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "table_columns = {1:'table',2:'country',3:'january',4:'february',5:'march',6:'april',7:'may',8:'june'}\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for row in tqdm(all_tables):\n",
    "    i+=1\n",
    "    row_name = str('RW'+str(i))\n",
    "    G.add_node(row_name , label= row_name, type='Row')\n",
    "    row_ids[row_name] = ' '.join([r for r in row])\n",
    "    id_rows[' '.join(row)] = row_name\n",
    "    \n",
    "    j=0\n",
    "    for cl in row:\n",
    "        j+=1\n",
    "        col_name = table_columns[j]\n",
    "        if cl == '': continue\n",
    "        if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "        n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(cl),K)]\n",
    "        for tg in n_grams:\n",
    "            G.add_node(tg,label=tg, type='Token')\n",
    "            G.add_edge(row_name,tg)\n",
    "            G.add_edge(col_name,tg)\n",
    "            \n",
    "\n",
    "\n",
    "i = 0\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "all_claims.update(user_claims)\n",
    "\n",
    "\n",
    "for claim in tqdm(all_claims):\n",
    "    i += 1\n",
    "    text = (claim)\n",
    "    claim_name = str('Claim'+str(i))\n",
    "    G.add_node(claim_name , label= claim_name, type='Claim')\n",
    "    claim_ids[claim_name] = claim\n",
    "    id_claim[claim] = claim_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "\n",
    "    for tg in n_grams:\n",
    "        if not G.has_node(tg):            \n",
    "            continue\n",
    "           \n",
    "        if not G.has_edge(claim_name,tg):            G.add_edge(claim_name,tg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:32:15.815575Z",
     "start_time": "2021-07-23T10:32:13.491128Z"
    },
    "id": "frJt1r-UMy8j"
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "for cl in claim_ids:\n",
    "    ground_truth[cl] = []\n",
    "    if claim_ids[cl] in user_claims:\n",
    "        for r in user_claims[claim_ids[cl]]:\n",
    "            for rr in id_rows:\n",
    "                if ' '.join(reversed(r)) in rr:             \n",
    "                    ground_truth[cl].append(id_rows[rr])\n",
    "    else:\n",
    "        for r in id_rows:\n",
    "            if ' '.join(all_claims[claim_ids[cl]][0:2]) in r:\n",
    "                ground_truth[cl].append(id_rows[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method0: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:33:03.649271Z",
     "start_time": "2021-07-23T10:33:03.640922Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from gensim.summarization.bm25 import BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:33:04.124182Z",
     "start_time": "2021-07-23T10:33:04.101460Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [row for row in all_tables]\n",
    "results = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:33:16.824304Z",
     "start_time": "2021-07-23T10:33:04.552062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7058/7058 [00:15<00:00, 455.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "query_BM25 = {}\n",
    "for query in tqdm(ground_truth):\n",
    "    text = claim_ids[query]\n",
    "    scores = results.get_scores(text.split())\n",
    "    arr = np.array(scores)\n",
    "    topK = arr.argsort()[::-1]\n",
    "    query_BM25[query] = [(corpus[idx],scores[idx]) for idx in topK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:33:37.647412Z",
     "start_time": "2021-07-23T10:33:16.825630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################### 1 ###########################\n",
      "\n",
      "MRR: 0.1412037037037037 MAP: 0.1388888888888889 HAS POSITIVE: 0.1412037037037037\n",
      "\n",
      "#################### 2 ###########################\n",
      "\n",
      "MRR: 0.21643518518518517 MAP: 0.21357783564814814 HAS POSITIVE: 0.2916666666666667\n",
      "\n",
      "#################### 3 ###########################\n",
      "\n",
      "MRR: 0.2578124999999971 MAP: 0.25444878472221955 HAS POSITIVE: 0.4157986111111111\n",
      "\n",
      "#################### 5 ###########################\n",
      "\n",
      "MRR: 0.31111111111111095 MAP: 0.3073929398148139 HAS POSITIVE: 0.6514756944444444\n",
      "\n",
      "#################### 10 ###########################\n",
      "\n",
      "MRR: 0.33897018298060017 MAP: 0.3352407591122872 HAS POSITIVE: 0.8310185185185185\n",
      "\n",
      "#################### 20 ###########################\n",
      "\n",
      "MRR: 0.33958778160395503 MAP: 0.3359237541769066 HAS POSITIVE: 0.8392650462962963\n",
      "\n",
      "#################### 50 ###########################\n",
      "\n",
      "MRR: 0.3398344050608905 MAP: 0.3361788879824261 HAS POSITIVE: 0.8470775462962963\n",
      "\n",
      "#################### 50000 ###########################\n",
      "\n",
      "MRR: 0.3403926014993229 MAP: 0.3367829371155561 HAS POSITIVE: 1.0\n"
     ]
    }
   ],
   "source": [
    "for KK in [1,2,3,5,10,20,50,50000]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for query in query_BM25:\n",
    "        if query not in ground_truth or len(ground_truth[query])==0: continue\n",
    "        if query in [id_claim[c] for c in user_claims]: continue\n",
    "            \n",
    "        i+=1\n",
    "        preds = [id_rows[' '.join(f)] for (f,j) in query_BM25[query]][0:KK]\n",
    "        golds = [g for g in ground_truth[query]]\n",
    "        \n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "# Method1: Unsupervised SentenceBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:33:40.975553Z",
     "start_time": "2021-07-23T10:33:40.941042Z"
    },
    "id": "itxiXUgAbOJs"
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "vocabs = set()\n",
    "for node in G.nodes():\n",
    "    if G.nodes()[node]['type'] == 'Token':\n",
    "        if len(node.split('_')) == 1: vocabs.add(node)\n",
    "\n",
    "def return_filtered(text):\n",
    "    text = remove_stopwords(normalize_text(text))\n",
    "    t = ''\n",
    "    for token in word_tokenize(text):\n",
    "        if token in vocabs: t += token + ' '\n",
    "    return t   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m252.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /workspace/venv/lib/python3.8/site-packages (from sentence_transformers) (4.25.1)\n",
      "Requirement already satisfied: tqdm in /workspace/venv/lib/python3.8/site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /workspace/venv/lib/python3.8/site-packages (from sentence_transformers) (1.13.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/33.8 MB\u001b[0m \u001b[31m16.4 kB/s\u001b[0m eta \u001b[36m0:33:13\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:33:44.021759Z",
     "start_time": "2021-07-23T10:33:41.798759Z"
    },
    "id": "fr3LfheWpA2c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:51:35.442309Z",
     "start_time": "2021-02-08T08:51:29.782292Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O30uNlqipCSq",
    "outputId": "e506aa94-6b4d-4c37-9d98-161e09466da2"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T08:48:16.080516Z",
     "start_time": "2021-02-07T08:48:16.075329Z"
    },
    "id": "okQTtgyEgvt3"
   },
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['[COL]','[VAL]']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T12:36:41.055257Z",
     "start_time": "2021-02-07T12:36:40.450627Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTc2I9kNIDLH",
    "outputId": "476acae1-4c14-42c4-befe-a33efcbe7acf"
   },
   "outputs": [],
   "source": [
    "word_embedding_model = model._first_module()   #Your models.Transformer object\n",
    "word_embedding_model.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T20:39:56.482883Z",
     "start_time": "2021-02-07T20:39:56.473854Z"
    },
    "id": "FMmg36woh-KW"
   },
   "outputs": [],
   "source": [
    "table_content = []\n",
    "for t in all_tables:\n",
    "  for c in range(0,len(t)):\n",
    "    text += ' [COL] ' + str(table_columns[c+1]) + ' [VAL] ' + str(t[c])\n",
    "  \n",
    "  table_content.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T22:08:14.824279Z",
     "start_time": "2021-02-07T22:06:32.957756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "88326c5ded924b7c82509df6d13266f4",
      "063de19dd82a4733a1cd405bb33496b2",
      "918cd1d9a3a04b44a9455d39c9d335bc",
      "c1b879f1457042bca653dbd80158e499",
      "14b1f7d37ad84702974bd7373b44bb6f",
      "ed64dfb33dd6496686ce0f7a2982fd6c",
      "ff774db2dd0c4a4da62f03302b5082c1",
      "85d2fb1ae59642f88fbf97246fcef519"
     ]
    },
    "id": "-Kyvdth_WJmb",
    "outputId": "09281204-eae5-4925-f783-00f0a899d1dd"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "claim_embs = model.encode([t for t in table_content],show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:38:21.898944Z",
     "start_time": "2021-02-07T22:09:37.507992Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdV18qpcpfJa",
    "outputId": "c4777cc2-c6b4-4bee-cb65-b7d46ae83792",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_quey = {}\n",
    "\n",
    "for claim in tqdm([g for g in all_claims.keys()],position=0):\n",
    "  text = claim \n",
    "\n",
    "  m_emb = model.encode(claim)\n",
    "  \n",
    "  temp = []\n",
    "  for rv in range(0,len(row_ids)):\n",
    "    temp.append(([r for r in row_ids.keys()][rv],cosine_similarity(m_emb.reshape(1, -1),claim_embs[rv].reshape(1, -1))[0][0]))\n",
    "  pred_quey[id_claim[claim]] = sorted(temp,key=lambda dist:dist[1],reverse=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T21:40:27.115795Z",
     "start_time": "2021-02-08T21:40:26.836901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48isJH_uRceX",
    "outputId": "a9848474-c51d-404c-8792-2a53a123c875"
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,500]: \n",
    "    i = 0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for query in pred_query:\n",
    "        if query not in ground_truth or len(ground_truth[query])==0: continue\n",
    "        if query not in [id_claim[c] for c in user_claims]: continue\n",
    "            \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in pred_query[query]][0:KK]\n",
    "        golds = [g for g in ground_truth[query]]\n",
    "        \n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx7qwG7xpfmY"
   },
   "source": [
    "# Method2: Supervised SentenceBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIsht5-vwosV"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "SBmodel = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:39:00.426637Z",
     "start_time": "2021-02-07T23:39:00.374239Z"
    },
    "id": "ovlInpZinkG9"
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:52:29.153509Z",
     "start_time": "2021-02-07T23:39:12.153527Z"
    },
    "id": "j9Tc9pVSrov5"
   },
   "outputs": [],
   "source": [
    "table_embs = {}\n",
    "for row in tqdm(id_rows,position=0):\n",
    "  table_embs[row] = model.encode(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:51:44.934302Z",
     "start_time": "2021-02-08T08:51:41.564954Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ao6Jk6oEsK_m",
    "outputId": "3654b088-b8ad-4b4f-8564-cc6590bb6e20"
   },
   "outputs": [],
   "source": [
    "query_score = []\n",
    "\n",
    "for claim in tqdm([g for g in all_claims.keys()],position=0):\n",
    "  text = claim\n",
    "    \n",
    "  m_emb = model.encode(text)\n",
    "    \n",
    "  for rw in id_rows:\n",
    "    temp = []\n",
    "    temp.append(cosine_similarity(m_emb.reshape(1, -1),table_embs[rw].reshape(1, -1))[0][0])\n",
    "      \n",
    "    if id_rows[rw] in ground_truth[claim]: temp.append(1)\n",
    "    else: temp.append(0)\n",
    "    query_score.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:09:36.021328Z",
     "start_time": "2021-02-08T01:09:32.345414Z"
    },
    "id": "3IGZ3HOqvh7_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.array(query_score)\n",
    "X = dataset[:,0:1]\n",
    "y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:09:36.107233Z",
     "start_time": "2021-02-08T01:09:36.022964Z"
    },
    "id": "BmM9G3tCvk9k"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import losses,optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=1, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(lr=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:18:21.302043Z",
     "start_time": "2021-02-08T01:09:36.108844Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "BebLkAxdvp1F",
    "outputId": "34ba187f-2e25-48a5-ea55-bb0ec27a12d7"
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1.,1: 500.}\n",
    "\n",
    "model.fit(X, y, epochs=50, batch_size=2048,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:35:42.889408Z",
     "start_time": "2021-02-08T08:35:21.556166Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "W7kl3keKvsWg",
    "outputId": "af02b32a-404e-45cd-a3c7-40b65bcd9587"
   },
   "outputs": [],
   "source": [
    "query_predictions = {}\n",
    "\n",
    "for query in tqdm(ground_truth,position=0):\n",
    "  if query not in [id_claim[c] for c in user_claims]: continue\n",
    "\n",
    "    \n",
    "  text = claim_ids[query]\n",
    "            \n",
    "  m_emb = SBmodel.encode(text)\n",
    "\n",
    "  seen = []\n",
    "  data,scores = [],[]\n",
    "\n",
    "  for rw in id_rows:\n",
    "    seen.append(rw)\n",
    "    data.append(cosine_similarity(m_emb.reshape(1, -1),table_embs[rw].reshape(1, -1))[0][0])\n",
    "\n",
    "  res = model.predict(np.array(data))\n",
    "        \n",
    "  for i in range(0,len(res)):\n",
    "      scores.append((seen[i],res[i][0]))\n",
    "        \n",
    "  query_predictions[query] = sorted(scores, key=lambda dist: dist[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:53:09.245287Z",
     "start_time": "2021-02-08T08:53:09.209278Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "pf2zTwg7BuaF",
    "outputId": "b94015eb-e002-48b7-acbc-da0e00a41772"
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,500]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for query in query_predictions:\n",
    "        if id_claim[query] not in ground_truth or len(ground_truth[id_claim[query]])==0: continue\n",
    "        if query in [id_claim[c] for c in user_claims]: continue\n",
    "        \n",
    "        i+=1\n",
    "        preds = [id_rows[f] for (f,j) in query_predictions[query]][0:KK]\n",
    "        golds = [f for f in ground_truth[id_claim[query]]]\n",
    "\n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yq3mJjHbMwY"
   },
   "source": [
    "# Method3: Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T21:50:04.461006Z",
     "start_time": "2021-02-08T21:50:01.897617Z"
    },
    "id": "p29YtWmiCcRf"
   },
   "outputs": [],
   "source": [
    "corona_SB = query_predictions\n",
    "corona_BM25 = query_BM25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T21:51:41.703541Z",
     "start_time": "2021-02-08T21:51:41.658906Z"
    }
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "data = dlib.ranking_pair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T21:59:35.024205Z",
     "start_time": "2021-02-08T21:55:20.540827Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "aulyl4NUfTEn",
    "outputId": "1fb56d02-4bcd-4db0-a3ea-4183c2e3f219"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "for query in tqdm(ground_truth,position=0):\n",
    "\n",
    "    m_BM = [' '.join(i) for (i,j) in corona_BM25[ query]]\n",
    "    m_SB = [i for (i,j) in corona_SB[query]]\n",
    "    \n",
    "    \n",
    "    for r in row_ids:\n",
    "        if r in ground_truth[query]: \n",
    "            data.relevant.append(dlib.vector([m_BM.index(row_ids[r])+1, m_SB.index(r)+1]))\n",
    "        else:  data.nonrelevant.append(dlib.vector([m_BM.index(row_ids[r])+1, m_SB.index(r)+1]))\n",
    "\n",
    "trainer = dlib.svm_rank_trainer()\n",
    "trainer.c = 1000\n",
    "\n",
    "rank = trainer.train(data)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T22:06:36.369832Z",
     "start_time": "2021-02-08T22:02:48.904687Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "i=0\n",
    "rerank_corona = {}\n",
    "\n",
    "for query in tqdm(ground_truth,position=0):\n",
    "    i+=1\n",
    "\n",
    "    m_BM = [' '.join(i) for (i,j) in corona_BM25[ query]]\n",
    "    m_SB = [i for (i,j) in corona_SB[query]]\n",
    "\n",
    "    temp = []\n",
    "    for r in row_ids:\n",
    "        temp.append((r,rank(dlib.vector([m_BM.index(row_ids[r])+1, m_SB.index(r)+1]))))\n",
    "    temp = sorted(temp, key=lambda dist: dist[1],reverse = True)\n",
    "    rerank_corona[query] = temp\n",
    "print((time.time()-st)/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T22:09:20.117566Z",
     "start_time": "2021-02-08T22:09:13.992500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "UAoA5hflvs78",
    "outputId": "7293ee00-40f9-4fbc-da7d-188abc64aa47"
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,500]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for query in rerank_corona:\n",
    "        if query not in ground_truth or len(ground_truth[query])==0: continue\n",
    "        if query in [id_claim[c] for c in user_claims]: continue\n",
    "        \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in rerank_corona[query]][0:KK]\n",
    "        golds = [f for f in ground_truth[query]]\n",
    "\n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method4: Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for query in all_tables:\n",
    "    text = ' '.join([str(m) for m in query]) \n",
    "    data.append(text)\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 50\n",
    "\n",
    "model = Doc2Vec(size=vec_size, min_count=10, dm =0, workers=1, window=4,seed=0, epochs=max_epochs\n",
    "                )\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "model.train(tagged_data, total_examples=model.corpus_count,epochs=model.epochs)\n",
    "\n",
    "\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "query_d2v = {}\n",
    "for query in tqdm(ground_truth):\n",
    "    text = claim_ids[query]\n",
    "    \n",
    "    query_d2v[query] = utils.cosine_distance(model,text,id_rows,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for KK in [1,5,20,500]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for query in query_d2v:\n",
    "        if claim_ids[query] not in ground_truth: continue\n",
    "        i+=1\n",
    "        preds = [id_review[f] for (f,j) in query_d2v[query]][0:KK]\n",
    "        golds = [f for f in ground_truth[row_ids[query]]]\n",
    "\n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MRR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SentenceBERT_Corona",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "063de19dd82a4733a1cd405bb33496b2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14b1f7d37ad84702974bd7373b44bb6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "85d2fb1ae59642f88fbf97246fcef519": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88326c5ded924b7c82509df6d13266f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_918cd1d9a3a04b44a9455d39c9d335bc",
       "IPY_MODEL_c1b879f1457042bca653dbd80158e499"
      ],
      "layout": "IPY_MODEL_063de19dd82a4733a1cd405bb33496b2"
     }
    },
    "918cd1d9a3a04b44a9455d39c9d335bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Batches: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed64dfb33dd6496686ce0f7a2982fd6c",
      "max": 37,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14b1f7d37ad84702974bd7373b44bb6f",
      "value": 37
     }
    },
    "c1b879f1457042bca653dbd80158e499": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85d2fb1ae59642f88fbf97246fcef519",
      "placeholder": "​",
      "style": "IPY_MODEL_ff774db2dd0c4a4da62f03302b5082c1",
      "value": " 37/37 [04:07&lt;00:00,  6.68s/it]"
     }
    },
    "ed64dfb33dd6496686ce0f7a2982fd6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff774db2dd0c4a4da62f03302b5082c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

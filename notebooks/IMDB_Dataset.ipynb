{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:29:56.768960Z",
     "start_time": "2021-02-08T10:29:56.156082Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from imdbUtils import *\n",
    "import csv\n",
    "pd.options.display.max_colwidth=500\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Movie Reveiws Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:29:58.349802Z",
     "start_time": "2021-02-08T10:29:58.339590Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load(open('data/imdb/imdb_reviews_1000film.df','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:30:20.666814Z",
     "start_time": "2021-02-08T10:30:20.027814Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "movies_dic = {}\n",
    "with open('data/imdb/imdb_movielens.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        if row[12].replace('_',' ') not in movies_dic: \n",
    "            movies_dic[row[12].replace('_',' ')] = []\n",
    "            \n",
    "        temp = [r.replace('_',' ') for r in row[0:10]]\n",
    "        \n",
    "        month,year = '',''\n",
    "        if len(row[10]) > 0:        \n",
    "            month = datetime.date(1900, int(row[10][4::]), 1).strftime('%B')\n",
    "            year = row[10][0:4]\n",
    "        \n",
    "        temp.append(month.lower() + ' ' + year)\n",
    "        temp.append(int(float(row[14])))\n",
    "        \n",
    "        movies_dic[row[12].replace('_',' ')].append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:30:25.235792Z",
     "start_time": "2021-02-08T10:30:21.659644Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import graphUtils\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:30:25.248616Z",
     "start_time": "2021-02-08T10:30:25.237670Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "def return_n_grams(text,k):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = set()\n",
    "    for i in range(0,len(tokens)-(k-1)):\n",
    "        n_grams.add( ' '.join( ( [tk for tk in tokens[i:i+k]]) ))\n",
    "        \n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def find_all_n_grams (text,n):\n",
    "    n_grams = []\n",
    "    for k in range(1,n+1):\n",
    "        k_grams = return_n_grams(text,k)\n",
    "        for g in k_grams: n_grams.append(g)\n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:30:25.269965Z",
     "start_time": "2021-02-08T10:30:25.251247Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:31:05.521773Z",
     "start_time": "2021-02-08T10:30:27.285142Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42206/42206 [00:38<00:00, 1104.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "\n",
    "i = 0\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for movie in tqdm(movies_dic):\n",
    "    i+=1\n",
    "    row_name = str('RW'+str(i))\n",
    "    G.add_node(row_name , label= row_name, type='Row')\n",
    "    row_ids[row_name] = movie\n",
    "    id_rows[movie] = row_name\n",
    "    j=0\n",
    "    \n",
    "    cols = [movie]\n",
    "    for c in movies_dic[movie]: cols.append(c)\n",
    "    for cl in cols:\n",
    "    \n",
    "#    for cl in movies_dic[movie]:\n",
    "        j+=1\n",
    "        col_name = str('CL'+str(i))\n",
    "        if cl == '': continue\n",
    "        if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "        n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(cl),K)]\n",
    "        \n",
    "        \n",
    "        for tg in n_grams:\n",
    "            \n",
    "            if not G.has_node(tg):\n",
    "                G.add_node(tg,label=tg, type='Token')\n",
    "            G.add_edge(row_name,tg)\n",
    "            G.add_edge(col_name,token_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:31:14.887818Z",
     "start_time": "2021-02-08T10:31:05.523664Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:09, 213.95it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "review_ids = {}\n",
    "id_review = {}\n",
    "\n",
    "for row in tqdm(df.itertuples()):\n",
    "    if row.movie.lower() not in movies_dic: continue\n",
    "    i += 1\n",
    "    text = remove_stopwords((row.user_review.lower()))\n",
    "    review_name = str('Review'+str(i))\n",
    "    G.add_node(review_name , label= review_name, type='Review')\n",
    "    review_ids[review_name] = row.user_review\n",
    "    id_review[text] = review_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "\n",
    "    for tg in n_grams:\n",
    "        \n",
    "        \n",
    "        if not G.has_node(tg):\n",
    "            continue\n",
    "            \n",
    "        if not G.has_edge(review_name,tg):            G.add_edge(review_name,tg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:31:15.067146Z",
     "start_time": "2021-02-08T10:31:14.889946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 11880.90it/s]\n"
     ]
    }
   ],
   "source": [
    "ground_truth = {}\n",
    "for row in tqdm(df.itertuples()):\n",
    "    if row.movie.lower() not in movies_dic:         continue\n",
    "    movie_name = row.movie.lower() \n",
    "    if movie_name not in ground_truth: ground_truth[movie_name] = []\n",
    "    #if remove_stopwords(row.user_review.lower()) not in id_review: continue\n",
    "    ground_truth[movie_name]. append(id_review[remove_stopwords((row.user_review.lower()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T10:52:14.728993Z",
     "start_time": "2021-02-04T10:52:14.720304Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def random_walk(node,l):\n",
    "    res = ''\n",
    "    \n",
    "    p = 0\n",
    "    chosen = node\n",
    "    \n",
    "    res += str(chosen)\n",
    "\n",
    "    while (p<l):\n",
    "        chosen = random.sample([n for n in nx.neighbors(G,chosen)],1)[0]\n",
    "        if G.nodes[chosen]['type'] in ['Review','Row', 'Column', 'Token']:\n",
    "            res += ' ' + str(chosen)\n",
    "        p+=1\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def generate_random_walks(k,l):\n",
    "    rws = []\n",
    "    \n",
    "    for i in tqdm(range(0,k),position=0):\n",
    "        for node in G.nodes():\n",
    "            if len([n for n in nx.neighbors(G,node)]) == 0:\n",
    "                continue\n",
    "            if G.nodes[node]['type'] in ['Review','Row']:#,'Column']:\n",
    "                rws.append(random_walk(node,l))\n",
    "    return rws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T16:09:22.379245Z",
     "start_time": "2021-02-05T12:46:27.154445Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [3:22:54<00:00, 608.75s/it]  \n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(200,l=30)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:31:43.919354Z",
     "start_time": "2021-02-08T10:31:43.915389Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T16:13:56.598674Z",
     "start_time": "2021-02-05T16:09:22.382053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875600/875600 [04:33<00:00, 3204.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T16:25:52.151409Z",
     "start_time": "2021-02-05T16:13:56.600446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n",
      "Model is Ready\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T10:41:19.195445Z",
     "start_time": "2021-02-04T09:49:05.661450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n",
      "Model is Ready\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "\n",
    "FTmodel = FastText(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "FTmodel.build_vocab(tagged_data)\n",
    "FTmodel.train(tagged_data, total_examples=FTmodel.corpus_count, epochs=FTmodel.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T10:50:28.968537Z",
     "start_time": "2021-02-08T10:50:01.433687Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 785/785 [00:27<00:00, 28.76it/s]\n"
     ]
    }
   ],
   "source": [
    "movie_reviews = {}\n",
    "for movie in tqdm(ground_truth):\n",
    "    m_id = id_rows[movie]\n",
    "    \n",
    "    movie_reviews[movie] = utils.distance_w2v (model,m_id,review_ids,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T11:07:25.367272Z",
     "start_time": "2021-02-08T11:07:24.207406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################### 1 ###########################\n",
      "\n",
      "MRR: 0.5261146496815287 MAP: 0.2624203821656051 HAS POSITIVE: 0.5261146496815287\n",
      "\n",
      "#################### 2 ###########################\n",
      "\n",
      "MRR: 0.5738853503184713 MAP: 0.3668789808917197 HAS POSITIVE: 0.621656050955414\n",
      "\n",
      "#################### 3 ###########################\n",
      "\n",
      "MRR: 0.5840764331210189 MAP: 0.3922505307855624 HAS POSITIVE: 0.6522292993630573\n",
      "\n",
      "#################### 5 ###########################\n",
      "\n",
      "MRR: 0.6015286624203818 MAP: 0.4193842887473456 HAS POSITIVE: 0.7273885350318471\n",
      "\n",
      "#################### 10 ###########################\n",
      "\n",
      "MRR: 0.6096769790718832 MAP: 0.43710924072389 HAS POSITIVE: 0.7872611464968153\n",
      "\n",
      "#################### 20 ###########################\n",
      "\n",
      "MRR: 0.6139999515087263 MAP: 0.4450565207258278 HAS POSITIVE: 0.8509554140127389\n",
      "\n",
      "#################### 50 ###########################\n",
      "\n",
      "MRR: 0.6159639997987735 MAP: 0.44909400602079713 HAS POSITIVE: 0.910828025477707\n",
      "\n",
      "#################### 50000 ###########################\n",
      "\n",
      "MRR: 0.6165930071062916 MAP: 0.4515625533853679 HAS POSITIVE: 1.0\n"
     ]
    }
   ],
   "source": [
    "for KK in [1,2,3,5,10,20,50,50000]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for movie in movie_reviews:\n",
    "        if movie not in ground_truth: continue\n",
    "        i+=1\n",
    "        preds = [f for (f,j) in movie_reviews[movie]][0:KK]\n",
    "        golds = [f for f in ground_truth[movie]]\n",
    "\n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MRR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

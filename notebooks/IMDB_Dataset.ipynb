{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from imdbUtils import *\n",
    "import csv\n",
    "pd.options.display.max_colwidth=500\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Movie Reveiws Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load(open('data/imdb/imdb_reviews.df','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "movies_dic = {}\n",
    "with open('data/imdb/imdb_movielens.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        if row[12].replace('_',' ') not in movies_dic: \n",
    "            movies_dic[row[12].replace('_',' ')] = []\n",
    "            \n",
    "        temp = [r.replace('_',' ') for r in row[0:10]]\n",
    "        \n",
    "        month,year = '',''\n",
    "        if len(row[10]) > 0:        \n",
    "            month = datetime.date(1900, int(row[10][4::]), 1).strftime('%B')\n",
    "            year = row[10][0:4]\n",
    "        \n",
    "        temp.append(month.lower() + ' ' + year)\n",
    "        temp.append(int(float(row[14])))\n",
    "        \n",
    "        movies_dic[row[12].replace('_',' ')].append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import graphUtils\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "def return_n_grams(text,k):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = set()\n",
    "    for i in range(0,len(tokens)-(k-1)):\n",
    "        n_grams.add( ' '.join( ( [tk for tk in tokens[i:i+k]]) ))\n",
    "        \n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def find_all_n_grams (text,n):\n",
    "    n_grams = []\n",
    "    for k in range(1,n+1):\n",
    "        k_grams = return_n_grams(text,k)\n",
    "        for g in k_grams: n_grams.append(g)\n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "K = 3\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for movie in tqdm(movies_dic):\n",
    "    for movie_row in movies_dic[movie]:\n",
    "        i+=1\n",
    "        row_name = str('RW'+str(i))\n",
    "        G.add_node(row_name , label= row_name, type='Row')\n",
    "        row_ids[row_name] = movie\n",
    "        id_rows[movie] = row_name\n",
    "    \n",
    "\n",
    "        cols = [movie]\n",
    "        for c in movies_dic[movie]: cols.append(c)\n",
    "        #for cl in cols:\n",
    "\n",
    "        for cl in movie_row:\n",
    "            j+=1\n",
    "            col_name = str('CL'+str(j))\n",
    "            if cl == '': continue\n",
    "            if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "            n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(cl),K)]\n",
    "            for tg in n_grams:\n",
    "                G.add_node(tg,label=tg, type='Token')\n",
    "                G.add_edge(row_name,tg)\n",
    "                G.add_edge(col_name,tg)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "review_ids = {}\n",
    "id_review = {}\n",
    "\n",
    "for row in tqdm(df.itertuples()):\n",
    "    if row.movie.lower() not in movies_dic: continue\n",
    "    i += 1\n",
    "    text = remove_stopwords(row.user_review.lower())\n",
    "    review_name = str('Review'+str(i))\n",
    "    G.add_node(review_name , label= review_name, type='Review')\n",
    "    review_ids[review_name] = row.user_review\n",
    "    id_review[row.user_review] = review_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "\n",
    "    for tg in n_grams:\n",
    "        if not G.has_node(tg):            \n",
    "            continue\n",
    "            #G.add_node(tg,label=tg, type='Token')\n",
    "            \n",
    "        if not G.has_edge(review_name,tg):            G.add_edge(review_name,tg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(30,l=20)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = {}\n",
    "for movie in tqdm(ground_truth):\n",
    "    m_id = id_rows[movie]\n",
    "    \n",
    "    movie_reviews[movie] = utils.distance_w2v (model,m_id,review_ids,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for KK in [1,2,3,5,10,20,50]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for movie in movie_reviews:\n",
    "        if movie not in ground_truth: continue\n",
    "        i+=1\n",
    "        preds = [f for (f,j) in movie_reviews[movie]][0:KK]\n",
    "        golds = [f for f in ground_truth[movie]]\n",
    "        \n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MRR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

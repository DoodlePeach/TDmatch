{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:12:32.421645Z",
     "start_time": "2021-07-23T10:12:30.031108Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "from nltk import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:12:32.847442Z",
     "start_time": "2021-07-23T10:12:32.422927Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from graphUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:12:33.428774Z",
     "start_time": "2021-07-23T10:12:33.384672Z"
    }
   },
   "outputs": [],
   "source": [
    "facts_info = pickle.load(open('../../data/politifact/politi_factsInfo.pkl','rb'))\n",
    "facts = pickle.load(open('../../data/politifact/politi_facts.pkl','rb'))\n",
    "all_claims = pickle.load(open('../../data/politifact/politi_claims.pkl','rb'))\n",
    "ground_truth = pickle.load(open('../../data/politifact/politi_GT.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:12:34.587113Z",
     "start_time": "2021-07-23T10:12:34.301741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "\n",
    "for claim in tqdm([g for g in ground_truth.keys()]):\n",
    "    node = remove_stopwords(normalize_text(claim))\n",
    "    i+=1\n",
    "\n",
    "    node_name = str('CLM'+str(i))\n",
    "    G.add_node(node_name , label= node_name, type='Claim')\n",
    "    nodes_labels[node_name] = node_name\n",
    "    claim_ids[node_name] = claim\n",
    "    id_claim[claim] = node_name\n",
    "                \n",
    "        \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "            \n",
    "        G.add_node(token,label=token, type='Token')\n",
    "        if not G.has_edge(node_name,token): G.add_edge(node_name,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:12:45.468375Z",
     "start_time": "2021-07-23T10:12:34.588816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fact_ids = {}\n",
    "id_fact = {}\n",
    "node_maps = []\n",
    "\n",
    "for fact in tqdm(facts):\n",
    "    node = remove_stopwords(normalize_text(facts_info[fact]))\n",
    "    i += 1\n",
    "    name = str('FCT'+ str(i))\n",
    "    \n",
    "    fact_ids[name] = node\n",
    "    id_fact[node] = name\n",
    "    \n",
    "    G.add_node(name,label = name, type='Fact')\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "        \n",
    "        if not G.has_node(token): continue\n",
    "        if not G.has_edge(name,token):            G.add_edge(name,token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:12:45.484339Z",
     "start_time": "2021-07-23T10:12:45.469730Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion with ConceptNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T14:17:35.378875Z",
     "start_time": "2021-06-22T14:17:35.331675Z"
    }
   },
   "outputs": [],
   "source": [
    "import conceptnet_lite\n",
    "conceptnet_lite.connect(\"../../data/conceptnet.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T14:52:52.496868Z",
     "start_time": "2021-06-22T14:17:35.749545Z"
    }
   },
   "outputs": [],
   "source": [
    "from conceptnet_lite import Label, edges_for\n",
    "from tqdm import tqdm\n",
    "for node in tqdm(G.copy().nodes()):\n",
    "    if G.nodes()[node]['type'] != 'Token': continue\n",
    "    \n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=G.nodes()[node]['label'].replace('_',' '), language='en').concepts, same_language=True):\n",
    "            if e.start.text == node:\n",
    "                new_node = e.end.text\n",
    "            else:\n",
    "                new_node = e.start.text\n",
    "            rel = e.relation.name\n",
    "            \n",
    "            for n in utils.normalize_text(new_node).split():\n",
    "                if not G.has_node(n):\n",
    "                    G.add_node(n, label = n, type = 'Token')\n",
    "            G.add_edge(node,n,type= rel)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "for n in G.copy().nodes():\n",
    "    if G.degree()[n] < 2:\n",
    "        G.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T23:34:49.436569Z",
     "start_time": "2021-07-12T23:34:49.398520Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSuM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:43:58.617243Z",
     "start_time": "2021-07-20T17:43:58.574771Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "node_ids = {}\n",
    "\n",
    "for n in G.nodes:\n",
    "    node_ids[n] = i\n",
    "    i+=1\n",
    "inv_nodes = {v: k for k, v in node_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:43:59.256322Z",
     "start_time": "2021-07-20T17:43:58.619910Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('../../politi_edgelist', 'w')\n",
    "\n",
    "for e in G.edges():    file.write(str(node_ids[e[0]]) + '\\t' + str(node_ids[e[1]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:45:01.080468Z",
     "start_time": "2021-07-20T17:45:01.025590Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../SSumM/output/summary_politi_edgelist.txt') as f:\n",
    "    sum_grapph = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "sum_grapph = [x.strip() for x in sum_grapph] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:45:02.241452Z",
     "start_time": "2021-07-20T17:45:01.745143Z"
    }
   },
   "outputs": [],
   "source": [
    "super_nodes,super_edges = {},[]\n",
    "edge_weights = {}\n",
    "\n",
    "for i in range(1,sum_grapph.index('<Superedge info>')):\n",
    "    node = sum_grapph[i].split('\\t')\n",
    "    idd = node[0]\n",
    "    node = [inv_nodes[int(n)] for n in node[1::]]\n",
    "    super_nodes[idd] = node\n",
    "\n",
    "for i in range(sum_grapph.index('<Superedge info>')+1,len(sum_grapph)):\n",
    "    e = sum_grapph[i].split('\\t')\n",
    "    if e[0] not in edge_weights:        edge_weights[e[0]] = {}\n",
    "    if e[1] not in edge_weights:        edge_weights[e[1]] = {}\n",
    "        \n",
    "\n",
    "    edge_weights[e[0]][e[1]] = e[2]\n",
    "    edge_weights[e[1]][e[0]] = e[2]\n",
    "    \n",
    "    super_edges.append((e[0],e[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:45:03.248144Z",
     "start_time": "2021-07-20T17:45:02.709743Z"
    }
   },
   "outputs": [],
   "source": [
    "SG = nx.Graph()\n",
    "\n",
    "for node in super_nodes:\n",
    "    name = ''\n",
    "    if ' '.join(super_nodes[node]).startswith(('Claim','Fact')):\n",
    "        name = ' '.join(super_nodes[node])\n",
    "    else:\n",
    "        name = super_nodes[node][0]\n",
    "        \n",
    "    SG.add_node(node , label= name, type='node')\n",
    "    \n",
    "for e in super_edges:\n",
    "    SG.add_edge(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:45:03.763361Z",
     "start_time": "2021-07-20T17:45:03.734320Z"
    }
   },
   "outputs": [],
   "source": [
    "G = SG\n",
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T12:05:42.976319Z",
     "start_time": "2021-07-19T12:05:40.743836Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in tqdm(G1.copy().nodes()):\n",
    "    if G1.degree()[n] < 2:\n",
    "        G1.remove_node(n)\n",
    "    elif G1.degree()[n] == 2:\n",
    "        ns = [n for n in nx.neighbors(G1,n)]\n",
    "        if not G1.has_edge(ns[0],ns[1]):\n",
    "            G1.add_edge(ns[0],ns[1])\n",
    "        G1.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T12:05:42.998405Z",
     "start_time": "2021-07-19T12:05:42.978787Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G1.nodes()),len(G1.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T10:14:33.719256Z",
     "start_time": "2021-07-20T07:44:38.283596Z"
    }
   },
   "outputs": [],
   "source": [
    "G1 = nx.Graph()\n",
    "\n",
    "from random import choice\n",
    "L = int(len(G.nodes())/4)\n",
    "sp = []\n",
    "i =0 \n",
    "pbar = tqdm(total=L,position=0)\n",
    "while i < L:\n",
    "    first = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Fact'])\n",
    "    second = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Claim'])\n",
    "    paths = nx.all_shortest_paths(G, first,second, weight=None)\n",
    "    for path in paths:\n",
    "        G1.add_nodes_from(path)\n",
    "        nx.add_path(G1,path)    \n",
    "    i+=1\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T10:14:33.850699Z",
     "start_time": "2021-07-20T10:14:33.721354Z"
    }
   },
   "outputs": [],
   "source": [
    "G = G1\n",
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-23T10:12:50.728Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(G,100,l=40)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T10:51:30.359577Z",
     "start_time": "2021-07-20T10:51:30.355609Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T11:02:21.271559Z",
     "start_time": "2021-07-20T10:51:30.362073Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T11:56:46.969924Z",
     "start_time": "2021-07-20T11:02:21.273935Z"
    }
   },
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=20, sg=1, seed=0, workers = 4)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T11:59:23.354505Z",
     "start_time": "2021-07-20T11:56:46.972728Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "claim_facts = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    if claim not in id_claim: continue\n",
    "    cl_id = id_claim[claim]\n",
    "    filtered_facts = {}\n",
    "    \n",
    "    if cl_id not in model.wv: continue\n",
    "    claim_facts[cl_id] = distance_w2v (model,cl_id,fact_ids,50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:02:23.690051Z",
     "start_time": "2021-07-20T12:02:15.796797Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,30000]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for claim in claim_facts:\n",
    "        if claim_ids[claim] not in ground_truth or len(ground_truth[claim_ids[claim]]) == 0: continue\n",
    "        \n",
    "        i+=1\n",
    "        preds = [fact_ids[f] for (f,j) in claim_facts[claim]][0:KK]\n",
    "        golds = [f for f in ground_truth[claim_ids[claim]]]\n",
    "\n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "\n",
    "\n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

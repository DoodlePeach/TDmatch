{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:14:52.381577Z",
     "start_time": "2021-07-23T10:14:52.363758Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from nltk import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:14:53.154298Z",
     "start_time": "2021-07-23T10:14:52.783674Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from graphUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:15:21.888090Z",
     "start_time": "2021-07-23T10:15:21.857938Z"
    }
   },
   "outputs": [],
   "source": [
    "ground_truth = pickle.load(open('../../data/snopes/snopes_GT.pkl','rb'))\n",
    "facts = pickle.load(open('../../data/snopes/snopes_facts.pkl','rb'))\n",
    "facts_info = pickle.load(open('../../data/snopes/snopes_facts_info.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:15:41.493973Z",
     "start_time": "2021-07-23T10:15:40.742511Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 2\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "\n",
    "for claim in tqdm(ground_truth):\n",
    "    node = remove_stopwords(normalize_text(claim))\n",
    "    i+=1\n",
    "\n",
    "    node_name = str('CLM'+str(i))\n",
    "    G.add_node(node_name , label= node_name, type='Claim')\n",
    "    nodes_labels[node_name] = node_name\n",
    "    claim_ids[node_name] = claim\n",
    "    id_claim[claim] = node_name\n",
    "                \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "            \n",
    "        G.add_node(token,label=token, type='Token')\n",
    "            \n",
    "        if not G.has_edge(node_name,token): G.add_edge(node_name,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:15:52.567916Z",
     "start_time": "2021-07-23T10:15:48.433683Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fact_ids = {}\n",
    "id_fact = {}\n",
    "new_nodes=0\n",
    "\n",
    "for fact in tqdm(facts):\n",
    "    node = remove_stopwords(normalize_text(facts_info[fact]))\n",
    "    i += 1\n",
    "    name = str('FCT'+ str(i))\n",
    "    \n",
    "    fact_ids[name] = node\n",
    "    id_fact[node] = name\n",
    "    \n",
    "    G.add_node(name,label = name, type='Fact')\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "           \n",
    "        \n",
    "        if not G.has_node(token): continue\n",
    "\n",
    "        if not G.has_edge(name,token):            G.add_edge(name,token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T07:47:38.869805Z",
     "start_time": "2021-07-21T07:47:38.855688Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(G.nodes),len(G.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion with ConceptNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-23T09:07:25.048Z"
    }
   },
   "outputs": [],
   "source": [
    "import conceptnet_lite\n",
    "# Address to conceptNet DB\n",
    "conceptnet_lite.connect(\"../../conceptnet.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T20:00:18.498353Z",
     "start_time": "2021-06-21T17:41:55.090662Z"
    }
   },
   "outputs": [],
   "source": [
    "from conceptnet_lite import Label, edges_for\n",
    "from tqdm import tqdm\n",
    "for node in tqdm(G.copy().nodes()):\n",
    "    if G.nodes()[node]['type'] != 'Token': continue\n",
    "    \n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=G.nodes()[node]['label'].replace('_',' '), language='en').concepts, same_language=True):\n",
    "            if e.start.text == node:\n",
    "                new_node = e.end.text\n",
    "            else:\n",
    "                new_node = e.start.text\n",
    "            rel = e.relation.name\n",
    "            \n",
    "            for n in utils.normalize_text(new_node).split():\n",
    "                if not G.has_node(n):\n",
    "                    G.add_node(n, label = n, type = 'Token')\n",
    "            G.add_edge(node,n,type= rel)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "for n in G.copy().nodes():\n",
    "    if G.degree()[n] < 2:\n",
    "        G.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T20:41:27.265342Z",
     "start_time": "2021-06-21T20:41:18.545810Z"
    }
   },
   "outputs": [],
   "source": [
    "#nx.write_graphml(G,'data/KnownLie/snopes_expanded.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T09:26:43.836930Z",
     "start_time": "2021-07-13T09:26:43.783064Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSuM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:46:34.471514Z",
     "start_time": "2021-07-20T17:46:34.366420Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "node_ids = {}\n",
    "\n",
    "for n in G.nodes:\n",
    "    node_ids[n] = i\n",
    "    i+=1\n",
    "inv_nodes = {v: k for k, v in node_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:46:35.759323Z",
     "start_time": "2021-07-20T17:46:34.474513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating input for SSuM\n",
    "file = open('snopes_edgelist', 'w')\n",
    "\n",
    "for e in G.edges():    file.write(str(node_ids[e[0]]) + '\\t' + str(node_ids[e[1]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:47:13.781666Z",
     "start_time": "2021-07-20T17:47:13.667823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading output of SSuM \n",
    "with open('../SSumM/output/summary_snopes_edgelist.txt') as f:\n",
    "    sum_grapph = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "sum_grapph = [x.strip() for x in sum_grapph] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:47:15.848570Z",
     "start_time": "2021-07-20T17:47:14.766935Z"
    }
   },
   "outputs": [],
   "source": [
    "super_nodes,super_edges = {},[]\n",
    "edge_weights = {}\n",
    "\n",
    "for i in range(1,sum_grapph.index('<Superedge info>')):\n",
    "    node = sum_grapph[i].split('\\t')\n",
    "    idd = node[0]\n",
    "    node = [inv_nodes[int(n)] for n in node[1::]]\n",
    "    super_nodes[idd] = node\n",
    "\n",
    "for i in range(sum_grapph.index('<Superedge info>')+1,len(sum_grapph)):\n",
    "    e = sum_grapph[i].split('\\t')\n",
    "    if e[0] not in edge_weights:        edge_weights[e[0]] = {}\n",
    "    if e[1] not in edge_weights:        edge_weights[e[1]] = {}\n",
    "        \n",
    "\n",
    "    edge_weights[e[0]][e[1]] = e[2]\n",
    "    edge_weights[e[1]][e[0]] = e[2]\n",
    "    \n",
    "    super_edges.append((e[0],e[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:47:17.575906Z",
     "start_time": "2021-07-20T17:47:16.361409Z"
    }
   },
   "outputs": [],
   "source": [
    "SG = nx.Graph()\n",
    "\n",
    "for node in super_nodes:\n",
    "    name = ''\n",
    "    if ' '.join(super_nodes[node]).startswith(('Claim','Fact')):\n",
    "        name = ' '.join(super_nodes[node])\n",
    "    else:\n",
    "        name = super_nodes[node][0]\n",
    "        \n",
    "    SG.add_node(node , label= name, type='node')\n",
    "    \n",
    "for e in super_edges:\n",
    "    SG.add_edge(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:47:18.051578Z",
     "start_time": "2021-07-20T17:47:18.014645Z"
    }
   },
   "outputs": [],
   "source": [
    "G = SG\n",
    "len(nodes()),len(edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge Rmoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:46:10.073942Z",
     "start_time": "2021-07-20T15:46:05.696774Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in tqdm(G.copy().nodes()):\n",
    "    if G.degree()[n] < 2:\n",
    "        G.remove_node(n)\n",
    "    elif G.degree()[n] == 2:\n",
    "        ns = [n for n in nx.neighbors(G,n)]\n",
    "        if not G.has_edge(ns[0],ns[1]):\n",
    "            G.add_edge(ns[0],ns[1])\n",
    "        G.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:46:10.113419Z",
     "start_time": "2021-07-20T15:46:10.075754Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.Graph()\n",
    "\n",
    "from random import choice\n",
    "L = int(len(G.nodes())/4)\n",
    "sp = []\n",
    "i =0 \n",
    "pbar = tqdm(total=L,position=0)\n",
    "while i < L:\n",
    "    first = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Fact'])\n",
    "    second = choice([n for n in G.nodes() if G.nodes()[n]['type'] == 'Claim'])\n",
    "    paths = nx.all_shortest_paths(G, first,second, weight=None)\n",
    "    for path in paths:\n",
    "        G1.add_nodes_from(path)\n",
    "        nx.add_path(G1,path)    \n",
    "    i+=1\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G1\n",
    "len(G.nodes()),len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-23T10:16:23.538Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(G,30,l=45)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:30:28.299853Z",
     "start_time": "2021-07-20T16:30:28.297210Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:45:46.391016Z",
     "start_time": "2021-07-20T16:30:28.301226Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:58:53.514801Z",
     "start_time": "2021-07-20T16:45:46.393178Z"
    }
   },
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=20, sg=0, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:01:43.865008Z",
     "start_time": "2021-07-20T16:58:53.516520Z"
    }
   },
   "outputs": [],
   "source": [
    "claim_facts = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    cl_id = id_claim[claim]\n",
    "    if cl_id not in model.wv: continue\n",
    "\n",
    "    \n",
    "    claim_facts[cl_id] = distance_w2v (model,cl_id,fact_ids,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:01:46.776369Z",
     "start_time": "2021-07-20T17:01:43.866552Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for KK in [1,5,20,10000]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MR, hasP = 0,0,0\n",
    "\n",
    "    for claim in claim_facts:\n",
    "        if claim_ids[claim] not in ground_truth or len(ground_truth[claim_ids[claim]]) == 0: continue\n",
    "        \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in claim_facts[claim]][0:KK]\n",
    "        golds = [id_fact[f] for f in ground_truth[claim_ids[claim]]]\n",
    "\n",
    "        \n",
    "        MAP += MAP_K(golds,preds)\n",
    "        MR += MRR(golds,preds)\n",
    "        hasP += HAS_POSITIVE(golds,preds)\n",
    "\n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

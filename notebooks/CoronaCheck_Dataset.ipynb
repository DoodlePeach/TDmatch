{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T11:41:57.410935Z",
     "start_time": "2021-02-08T11:41:56.879640Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from imdbUtils import *\n",
    "import csv\n",
    "pd.options.display.max_colwidth=500\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corona Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T11:41:58.003270Z",
     "start_time": "2021-02-08T11:41:57.961322Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "all_tables = []\n",
    "\n",
    "directory = 'data/Corona/tables/'\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        with open(directory + file) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=',')\n",
    "            \n",
    "            next(spamreader)\n",
    "            for row in spamreader:\n",
    "                temp = [file.split('.')[0].replace('_',' ')]\n",
    "                temp.append(row[0].lower())\n",
    "                for r in row[-6::]: temp.append(str(int(float(r))))\n",
    "                all_tables.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:03:07.771995Z",
     "start_time": "2021-02-07T13:03:07.768546Z"
    }
   },
   "outputs": [],
   "source": [
    "table_columns = {1:'table',2:'country',3:'january',4:'february',5:'march',6:'april',7:'may',8:'june'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:03:10.154012Z",
     "start_time": "2021-02-07T13:03:07.993452Z"
    }
   },
   "outputs": [],
   "source": [
    "all_claims = {}\n",
    "\n",
    "with open('data/Corona/Claims_English.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for r in csv_reader:\n",
    "        if r[-1] == 'a':\n",
    "            all_claims[r[0].lower()] = [r[1].replace('_',' '),r[2].lower(),r[3].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:03:10.571239Z",
     "start_time": "2021-02-07T13:03:10.518813Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('corona_GEN.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for claim in all_claims:\n",
    "        temp = [claim]\n",
    "        for c in all_claims[claim][0:4]: temp.append(c)\n",
    "        spamwriter.writerow(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:03:10.741652Z",
     "start_time": "2021-02-07T13:03:10.734582Z"
    }
   },
   "outputs": [],
   "source": [
    "user_claims = {}\n",
    "\n",
    "with open('data/Corona/Claims_users.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for r in csv_reader:\n",
    "        user_claims[r[0].lower()] = []        \n",
    "        for rr in r[1].split('-'):\n",
    "            for rrr in r[2].split('-'):\n",
    "                temp = [rr.lower(),rrr.replace('_',' ').lower()]\n",
    "                for rrrr in r[3::]:\n",
    "                    temp.append(rrrr.lower().replace('_',' '))\n",
    "            user_claims[r[0].lower()].append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:03:17.111913Z",
     "start_time": "2021-02-07T13:03:13.143761Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import graphUtils\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:03:17.132454Z",
     "start_time": "2021-02-07T13:03:17.114063Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "def return_n_grams(text,k):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = set()\n",
    "    for i in range(0,len(tokens)-(k-1)):\n",
    "        n_grams.add( ' '.join( ( [tk for tk in tokens[i:i+k]]) ))\n",
    "        \n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def find_all_n_grams (text,n):\n",
    "    n_grams = []\n",
    "    for k in range(1,n+1):\n",
    "        k_grams = return_n_grams(text,k)\n",
    "        for g in k_grams: n_grams.append(g)\n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T11:42:16.865941Z",
     "start_time": "2021-02-08T11:42:16.187609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ee4854e73a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "\n",
    "i = 0\n",
    "nodes_labels = {}\n",
    "row_ids = {}\n",
    "id_rows = {}\n",
    "\n",
    "for row in tqdm(all_tables):\n",
    "    i+=1\n",
    "    row_name = str('RW'+str(i))\n",
    "    G.add_node(row_name , label= row_name, type='Row')\n",
    "    row_ids[row_name] = ' '.join([r for r in row])\n",
    "    id_rows[' '.join(row)] = row_name\n",
    "    \n",
    "    j=0\n",
    "    for cl in row:\n",
    "        j+=1\n",
    "        col_name = table_columns[j]\n",
    "                \n",
    "        if not G.has_node(col_name):     G.add_node(col_name , label= col_name, type='Column')\n",
    "        n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(str(cl),K)]\n",
    "        \n",
    "        for tg in n_grams:\n",
    "            if not G.has_node(tg): G.add_node(tg,label=tg, type='Token')\n",
    "            G.add_edge(row_name,tg)\n",
    "            G.add_edge(col_name,tg)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:05:39.594624Z",
     "start_time": "2021-02-07T13:05:37.345684Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7058/7058 [00:02<00:00, 3167.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "i = 0\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "all_claims.update(user_claims)\n",
    "new_nodes = 0\n",
    "\n",
    "for claim in tqdm(all_claims):\n",
    "    i += 1\n",
    "    text = remove_stopwords(claim)\n",
    "    claim_name = str('Claim'+str(i))\n",
    "    G.add_node(claim_name , label= claim_name, type='Claim')\n",
    "    claim_ids[claim_name] = claim\n",
    "    id_claim[claim] = claim_name\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in find_all_n_grams(text,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "        if not G.has_node(token): continue\n",
    "\n",
    "        if not G.has_edge(claim_name,token):            G.add_edge(claim_name,token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T13:05:49.056066Z",
     "start_time": "2021-02-07T13:05:45.584699Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "for cl in claim_ids:\n",
    "    ground_truth[cl] = []\n",
    "    if claim_ids[cl] in user_claims:\n",
    "        for r in user_claims[claim_ids[cl]]:\n",
    "            for rr in id_rows:\n",
    "                if ' '.join(reversed(r)) in rr:             \n",
    "                    ground_truth[cl].append(id_rows[rr])\n",
    "    else:\n",
    "        for r in id_rows:\n",
    "            if ' '.join(all_claims[claim_ids[cl]][0:2]) in r:\n",
    "                ground_truth[cl].append(id_rows[r])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_walk(node,l):\n",
    "    res = ''\n",
    "    \n",
    "    p = 0\n",
    "    chosen = node\n",
    "    \n",
    "    res += str(chosen)\n",
    "\n",
    "    while (p<l):\n",
    "        chosen = random.sample([n for n in nx.neighbors(G,chosen)],1)[0]\n",
    "        #if G.nodes[chosen]['type'] in ['Claim','Row','Column']:\n",
    "        res += ' ' + str(chosen)\n",
    "        p+=1\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_random_walks(k,l):\n",
    "    rws = []\n",
    "    \n",
    "    for i in tqdm(range(0,k),position=0):\n",
    "        for node in G.nodes():\n",
    "            if len([n for n in nx.neighbors(G,node)]) == 0:\n",
    "                continue\n",
    "            #if G.nodes[node]['type'] in ['Claim','Row','Column']:\n",
    "            rws.append(random_walk(node,l))\n",
    "    return rws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:43<00:00,  5.15s/it]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(200,l=25)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204360/204360 [00:40<00:00, 5009.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n",
      "Model is Ready\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 30\n",
    "vec_size = 300\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=3, sg=1, seed=0, workers = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7058/7058 [02:46<00:00, 42.32it/s]\n"
     ]
    }
   ],
   "source": [
    "claims_rows = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    if claim in model.wv:\n",
    "        claims_rows[claim] = utils.distance_w2v (model,claim,row_ids,len(row_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################### 1 ###########################\n",
      "\n",
      "MRR: 0.5978448275862069 MAP: 0.5969109195402299 HAS POSITIVE: 0.5978448275862069\n",
      "\n",
      "#################### 2 ###########################\n",
      "\n",
      "MRR: 0.6861350574712644 MAP: 0.6845785440613027 HAS POSITIVE: 0.7744252873563219\n",
      "\n",
      "#################### 3 ###########################\n",
      "\n",
      "MRR: 0.7122365900383157 MAP: 0.7106561302682013 HAS POSITIVE: 0.8527298850574713\n",
      "\n",
      "#################### 5 ###########################\n",
      "\n",
      "MRR: 0.731525383141762 MAP: 0.7298503352490421 HAS POSITIVE: 0.9356321839080459\n",
      "\n",
      "#################### 10 ###########################\n",
      "\n",
      "MRR: 0.7374007366356504 MAP: 0.7358130359423467 HAS POSITIVE: 0.9758620689655172\n",
      "\n",
      "#################### 20 ###########################\n",
      "\n",
      "MRR: 0.7384300619888818 MAP: 0.736839039598753 HAS POSITIVE: 0.9895114942528735\n",
      "\n",
      "#################### 50 ###########################\n",
      "\n",
      "MRR: 0.7386324916487678 MAP: 0.7370564363430161 HAS POSITIVE: 0.9952586206896552\n",
      "\n",
      "#################### 1158 ###########################\n",
      "\n",
      "MRR: 0.7386818377012004 MAP: 0.7371654615970754 HAS POSITIVE: 1.0\n"
     ]
    }
   ],
   "source": [
    "for KK in [1,2,3,5,10,20,50,len(row_ids)]: \n",
    "    \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for claim in claims_rows:\n",
    "        if claim not in ground_truth or len(ground_truth[claim])==0: continue\n",
    "        #if claim in [id_claim[c] for c in user_claims]: continue\n",
    "            \n",
    "        i+=1\n",
    "        preds = [f for (f,j) in claims_rows[claim]][0:KK]\n",
    "        golds = [g for g in ground_truth[claim]]\n",
    "        \n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MRR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "        \n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    try:\n",
    "        print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)\n",
    "    except:\n",
    "        print('y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

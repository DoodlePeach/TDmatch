{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:27:12.029008Z",
     "start_time": "2021-02-07T23:27:12.024048Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:27:12.850467Z",
     "start_time": "2021-02-07T23:27:12.846573Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "ps = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:27:18.641973Z",
     "start_time": "2021-02-07T23:27:13.355936Z"
    }
   },
   "outputs": [],
   "source": [
    "facts_info = {}\n",
    "facts = []\n",
    "with open('../data/KnownLie/politifact/verified-claims/facts_with_articles.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        facts.append(row[1])\n",
    "        facts_info[row[1]] = remove_stopwords(utils.normalize_text(' '.join([row[1],row[6]])))\n",
    "        #facts_info[row[1]] = remove_stopwords(utils.normalize_text(row[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T22:29:53.496188Z",
     "start_time": "2021-02-07T22:29:53.196941Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from gensim.summarization.bm25 import BM25\n",
    "corpus = [f.split(\" \") for f in facts]\n",
    "results = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:38:51.107737Z",
     "start_time": "2021-02-08T08:38:31.973673Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "all_claims = []\n",
    "\n",
    "directory = '../data/KnownLie/politifact/fact-checking/'\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        with open(directory + file) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=',')\n",
    "            \n",
    "            next(spamreader)\n",
    "            for row in spamreader:\n",
    "                text = row[4]\n",
    "                scores = results.get_scores(text.split())\n",
    "                idx = scores.index(max(scores))\n",
    "                text = ' '.join(corpus[idx])\n",
    "                \n",
    "                if row[3] == 'Carbon copies': title = row[2].lower().replace(',',' ')\n",
    "                else:  title = row[3].lower().replace(',',' ')\n",
    "                title = title\n",
    "                if title not in ground_truth:\n",
    "                    ground_truth[title] = []\n",
    "                all_claims.append(text)\n",
    "                #ground_truth[title].append((text,row[5]))\n",
    "                ground_truth[title].append(facts_info[text])\n",
    "\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:32:42.977575Z",
     "start_time": "2021-02-07T23:32:42.747842Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import graphUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:32:43.051550Z",
     "start_time": "2021-02-07T23:32:43.046730Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:32:45.221504Z",
     "start_time": "2021-02-07T23:32:44.839593Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "G=nx.Graph()\n",
    "K = 3\n",
    "i = 0\n",
    "claim_ids = {}\n",
    "id_claim = {}\n",
    "\n",
    "for claim in tqdm([g for g in ground_truth.keys()]):\n",
    "    node = remove_stopwords(utils.normalize_text(claim))\n",
    "    i+=1\n",
    "\n",
    "    node_name = str('CLM'+str(i))\n",
    "    G.add_node(node_name , label= node_name, type='Claim')\n",
    "    nodes_labels[node_name] = node_name\n",
    "    claim_ids[node_name] = claim\n",
    "    id_claim[claim] = node_name\n",
    "                \n",
    "        \n",
    "    n_grams = [gr.replace(' ','_') for gr in graphUtils.find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "            \n",
    "        G.add_node(token,label=token, type='Token')\n",
    "            \n",
    "        if not G.has_edge(node_name,token): G.add_edge(node_name,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:33:06.113536Z",
     "start_time": "2021-02-07T23:32:46.396379Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fact_ids = {}\n",
    "id_fact = {}\n",
    "\n",
    "for fact in tqdm(facts):\n",
    "    node = remove_stopwords(utils.normalize_text(facts_info[fact]))\n",
    "    i += 1\n",
    "    name = str('FCT'+ str(i))\n",
    "    \n",
    "    fact_ids[name] = fact\n",
    "    id_fact[fact] = name\n",
    "    \n",
    "    G.add_node(name,label = name, type='Fact')\n",
    "    \n",
    "    n_grams = [gr.replace(' ','_') for gr in graphUtils.find_all_n_grams(node,K)]\n",
    "    n_grams = sorted(n_grams, key=lambda dist: len(dist),reverse = True)\n",
    "    \n",
    "    for tg in n_grams:\n",
    "        token = tg\n",
    "        \n",
    "        if not G.has_node(token): continue\n",
    "\n",
    "        if not G.has_edge(name,token):            G.add_edge(name,token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [facts_info[f].split() for f in facts]\n",
    "results = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "claim_facts_BM25 = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    cl_id = id_claim[claim]\n",
    "    scores = results.get_scores(remove_stopwords(utils.normalize_text(claim)).split())\n",
    "    arr = np.array(scores)\n",
    "    topK = tops = arr.argsort()[::-1]\n",
    "    claim_facts_BM25[cl_id] = [(id_fact[facts[idx]],scores[idx]) for idx in topK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomWalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_walk(node,l):\n",
    "    res = ''\n",
    "    \n",
    "    p = 0\n",
    "    chosen = node\n",
    "    \n",
    "    res += str(chosen)\n",
    "\n",
    "    while (p<l):\n",
    "        chosen = random.sample([n for n in nx.neighbors(G,chosen)],1)[0]\n",
    "        if G.nodes[chosen]['type'] in ['Claim','Fact','Token']:\n",
    "            res += ' ' + str(chosen)\n",
    "        p+=1\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_random_walks(k,l):\n",
    "    rws = []\n",
    "    \n",
    "    for i in tqdm(range(0,k),position=0):\n",
    "        for node in G.nodes():\n",
    "            if len([n for n in nx.neighbors(G,node)]) == 0:\n",
    "                continue\n",
    "            if G.nodes[node]['type'] in ['Claim','Fact']:\n",
    "                rws.append(random_walk(node,l))\n",
    "    return rws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "random_paths = generate_random_walks(200,l=25)\n",
    "for p in random_paths:\n",
    "    docs.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T23:28:58.768099Z",
     "start_time": "2021-02-07T23:28:58.763305Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm \n",
    "tagged_data = []\n",
    "for d in tqdm(docs,position=0):\n",
    "    tagged_data.append(word_tokenize(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "\n",
    "model = Word2Vec(size=vec_size, min_count=10, window=20, sg=1, seed=0, workers = 4)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Model is Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "%env PYTHONHASHSEED=0\n",
    "\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "\n",
    "FTmodel = FastText(size=vec_size,\n",
    "                min_count=10, \n",
    "                window=20,\n",
    "                 sg=0,\n",
    "                seed=0,\n",
    "               workers = 4)\n",
    "\n",
    "\n",
    "FTmodel.build_vocab(tagged_data)\n",
    "\n",
    "FTmodel.train(tagged_data, total_examples=FTmodel.corpus_count, epochs=FTmodel.epochs)\n",
    "\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:38:55.681040Z",
     "start_time": "2021-02-08T08:38:55.672261Z"
    }
   },
   "outputs": [],
   "source": [
    "gold_facts = {}\n",
    "for claim in ground_truth:\n",
    "    if claim not in gold_facts:\n",
    "        gold_facts[claim] = []\n",
    "    for fact in ground_truth[claim]:\n",
    "        if fact[0] in id_fact:\n",
    "            gold_facts[claim].append(id_fact[fact[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T02:00:39.060681Z",
     "start_time": "2021-02-07T23:35:15.484993Z"
    }
   },
   "outputs": [],
   "source": [
    "claim_facts = {}\n",
    "for claim in tqdm(ground_truth):\n",
    "    if claim not in id_claim: continue\n",
    "    cl_id = id_claim[claim]\n",
    "    filtered_facts = {}\n",
    "    \n",
    "    if cl_id not in model.wv: continue\n",
    "    claim_facts[cl_id] = utils.distance_w2v (model,cl_id,fact_ids,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:43:39.331925Z",
     "start_time": "2021-02-08T08:43:32.976634Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for KK in [1,3,5,10,20,50,30000]: \n",
    "    i = 0\n",
    "    precision,recall,fs = 0,0,0\n",
    "    MAP, MRR, hasP = 0,0,0\n",
    "\n",
    "    for claim in claim_facts:\n",
    "        if claim_ids[claim] not in ground_truth or len(ground_truth[claim_ids[claim]]) == 0: continue\n",
    "        \n",
    "        i+=1\n",
    "        preds = [f[0] for (f,j) in claim_facts[claim]][0:KK]\n",
    "        golds = [f for f in ground_truth[claim_ids[claim]]]\n",
    "\n",
    "        MAP += utils.MAP_K(golds,preds)\n",
    "        MRR += utils.MRR(golds,preds)\n",
    "        hasP += utils.HAS_POSITIVE(golds,preds)\n",
    "\n",
    "\n",
    "    print('\\n#################### ' + str(KK) + ' ###########################\\n')\n",
    "    print('MRR:',MRR/i,'MAP:',MAP/i, 'HAS POSITIVE:', hasP/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}